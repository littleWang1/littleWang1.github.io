[ { "title": "MVCC", "url": "/posts/MVCC/", "categories": "测试", "tags": "学习", "date": "2024-04-03 02:34:00 +0000", "snippet": "1.数据库操作数据流程图2.隐藏字段3.undolog3.1undolog有两个作用 数据回滚。当执行数据库指令的时候发生错误，我们需要需要回滚数据 MCVCC（undolog版本连）3.24.MVCC实现原理4.1readview", "content": "1.数据库操作数据流程图2.隐藏字段3.undolog3.1undolog有两个作用 数据回滚。当执行数据库指令的时候发生错误，我们需要需要回滚数据 MCVCC（undolog版本连）3.24.MVCC实现原理4.1readview" }, { "title": "分布式搜索引擎03", "url": "/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E03/", "categories": "测试", "tags": "学习", "date": "2024-03-12 02:34:00 +0000", "snippet": "分布式搜索引擎030.学习目标1.数据聚合聚合（aggregations）可以让我们极其方便的实现对数据的统计、分析、运算。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。1.1.聚合的种类聚合常见的有三类： 桶（Bucket）聚合：用来对...", "content": "分布式搜索引擎030.学习目标1.数据聚合聚合（aggregations）可以让我们极其方便的实现对数据的统计、分析、运算。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。1.1.聚合的种类聚合常见的有三类： 桶（Bucket）聚合：用来对文档做分组 TermAggregation：按照文档字段值分组，例如按照品牌值分组、按照国家分组 Date Histogram：按照日期阶梯分组，例如一周为一组，或者一月为一组 度量（Metric）聚合：用以计算一些值，比如：最大值、最小值、平均值等 Avg：求平均值 Max：求最大值 Min：求最小值 Stats：同时求max、min、avg、sum等 管道（pipeline）聚合：其它聚合的结果为基础做聚合 注意：参加聚合的字段必须是keyword、日期、数值、布尔类型1.2.DSL实现聚合现在，我们要统计所有数据中的酒店品牌有几种，其实就是按照品牌对数据分组。此时可以根据酒店品牌的名称做聚合，也就是Bucket聚合。1.2.1.Bucket聚合语法语法如下：GET /hotel/_search{  \"size\": 0,  // 设置size为0，结果中不包含文档，只包含聚合结果  \"aggs\": { // 定义聚合    \"brandAgg\": { //给聚合起个名字      \"terms\": { // 聚合的类型，按照品牌值聚合，所以选择term        \"field\": \"brand\", // 参与聚合的字段        \"size\": 20 // 希望获取的聚合结果数量      }    }  }}结果如图：1.2.2.聚合结果排序默认情况下，Bucket聚合会统计Bucket内的文档数量，记为_count，并且按照_count降序排序。我们可以指定order属性，自定义聚合的排序方式：GET /hotel/_search{  \"size\": 0,   \"aggs\": {    \"brandAgg\": {      \"terms\": {        \"field\": \"brand\",        \"order\": {          \"_count\": \"asc\" // 按照_count升序排列        },        \"size\": 20      }    }  }}1.2.3.限定聚合范围默认情况下，Bucket聚合是对索引库的所有文档做聚合，但真实场景下，用户会输入搜索条件，因此聚合必须是对搜索结果聚合。那么聚合必须添加限定条件。我们可以限定要聚合的文档范围，只要添加query条件即可：GET /hotel/_search{  \"query\": {    \"range\": {      \"price\": {        \"lte\": 200 // 只对200元以下的文档聚合      }    }  },   \"size\": 0,   \"aggs\": {    \"brandAgg\": {      \"terms\": {        \"field\": \"brand\",        \"size\": 20      }    }  }}这次，聚合得到的品牌明显变少了：1.2.4.Metric聚合语法上节课，我们对酒店按照品牌分组，形成了一个个桶。现在我们需要对桶内的酒店做运算，获取每个品牌的用户评分的min、max、avg等值。这就要用到Metric聚合了，例如stat聚合：就可以获取min、max、avg等结果。语法如下：GET /hotel/_search{  \"size\": 0,   \"aggs\": {    \"brandAgg\": {       \"terms\": {         \"field\": \"brand\",         \"size\": 20      },      \"aggs\": { // 是brands聚合的子聚合，也就是分组后对每组分别计算        \"score_stats\": { // 聚合名称          \"stats\": { // 聚合类型，这里stats可以计算min、max、avg等            \"field\": \"score\" // 聚合字段，这里是score          }        }      }    }  }}这次的score_stats聚合是在brandAgg的聚合内部嵌套的子聚合。因为我们需要在每个桶分别计算。另外，我们还可以给聚合结果做个排序，例如按照每个桶的酒店平均分做排序：1.2.5.小结aggs代表聚合，与query同级，此时query的作用是？ 限定聚合的的文档范围聚合必须的三要素： 聚合名称 聚合类型 聚合字段聚合可配置属性有： size：指定聚合结果数量 order：指定聚合结果排序方式 field：指定聚合字段1.3.RestAPI实现聚合1.3.1.API语法聚合条件与query条件同级别，因此需要使用request.source()来指定聚合条件。聚合条件的语法：聚合的结果也与查询结果不同，API也比较特殊。不过同样是JSON逐层解析：1.3.2.业务需求需求：搜索页面的品牌、城市等信息不应该是在页面写死，而是通过聚合索引库中的酒店数据得来的：分析：目前，页面的城市列表、星级列表、品牌列表都是写死的，并不会随着搜索结果的变化而变化。但是用户搜索条件改变时，搜索结果会跟着变化。例如：用户搜索“东方明珠”，那搜索的酒店肯定是在上海东方明珠附近，因此，城市只能是上海，此时城市列表中就不应该显示北京、深圳、杭州这些信息了。也就是说，搜索结果中包含哪些城市，页面就应该列出哪些城市；搜索结果中包含哪些品牌，页面就应该列出哪些品牌。如何得知搜索结果中包含哪些品牌？如何得知搜索结果中包含哪些城市？使用聚合功能，利用Bucket聚合，对搜索结果中的文档基于品牌分组、基于城市分组，就能得知包含哪些品牌、哪些城市了。因为是对搜索结果聚合，因此聚合是限定范围的聚合，也就是说聚合的限定条件跟搜索文档的条件一致。查看浏览器可以发现，前端其实已经发出了这样的一个请求：请求参数与搜索文档的参数完全一致。返回值类型就是页面要展示的最终结果：结果是一个Map结构： key是字符串，城市、星级、品牌、价格 value是集合，例如多个城市的名称1.3.3.业务实现在cn.itcast.hotel.web包的HotelController中添加一个方法，遵循下面的要求： 请求方式：POST 请求路径：/hotel/filters 请求参数：RequestParams，与搜索文档的参数一致 返回值类型：Map&lt;String, List&lt;String&gt;&gt;代码： @PostMapping(\"filters\") public Map&lt;String, List&lt;String&gt;&gt; getFilters(@RequestBody RequestParams params){ return hotelService.getFilters(params); }这里调用了IHotelService中的getFilters方法，尚未实现。在cn.itcast.hotel.service.IHotelService中定义新方法：Map&lt;String, List&lt;String&gt;&gt; filters(RequestParams params);在cn.itcast.hotel.service.impl.HotelService中实现该方法：@Overridepublic Map&lt;String, List&lt;String&gt;&gt; filters(RequestParams params) { try { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query buildBasicQuery(params, request); // 2.2.设置size request.source().size(0); // 2.3.聚合 buildAggregation(request); // 3.发出请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析结果 Map&lt;String, List&lt;String&gt;&gt; result = new HashMap&lt;&gt;(); Aggregations aggregations = response.getAggregations(); // 4.1.根据品牌名称，获取品牌结果 List&lt;String&gt; brandList = getAggByName(aggregations, \"brandAgg\"); result.put(\"品牌\", brandList); // 4.2.根据品牌名称，获取品牌结果 List&lt;String&gt; cityList = getAggByName(aggregations, \"cityAgg\"); result.put(\"城市\", cityList); // 4.3.根据品牌名称，获取品牌结果 List&lt;String&gt; starList = getAggByName(aggregations, \"starAgg\"); result.put(\"星级\", starList); return result; } catch (IOException e) { throw new RuntimeException(e); }}private void buildAggregation(SearchRequest request) { request.source().aggregation(AggregationBuilders .terms(\"brandAgg\") .field(\"brand\") .size(100) ); request.source().aggregation(AggregationBuilders .terms(\"cityAgg\") .field(\"city\") .size(100) ); request.source().aggregation(AggregationBuilders .terms(\"starAgg\") .field(\"starName\") .size(100) );}private List&lt;String&gt; getAggByName(Aggregations aggregations, String aggName) { // 4.1.根据聚合名称获取聚合结果 Terms brandTerms = aggregations.get(aggName); // 4.2.获取buckets List&lt;? extends Terms.Bucket&gt; buckets = brandTerms.getBuckets(); // 4.3.遍历 List&lt;String&gt; brandList = new ArrayList&lt;&gt;(); for (Terms.Bucket bucket : buckets) { // 4.4.获取key String key = bucket.getKeyAsString(); brandList.add(key); } return brandList;}2.自动补全当用户在搜索框输入字符时，我们应该提示出与该字符有关的搜索项，如图：这种根据用户输入的字母，提示完整词条的功能，就是自动补全了。因为需要根据拼音字母来推断，因此要用到拼音分词功能。2.1.拼音分词器要实现根据字母做补全，就必须对文档按照拼音分词。在GitHub上恰好有elasticsearch的拼音分词插件。地址：https://github.com/medcl/elasticsearch-analysis-pinyin课前资料中也提供了拼音分词器的安装包：安装方式与IK分词器一样，分三步：​\t①解压​\t②上传到虚拟机中，elasticsearch的plugin目录​\t③重启elasticsearch​\t④测试详细安装步骤可以参考IK分词器的安装过程。测试用法如下：POST /_analyze{  \"text\": \"如家酒店还不错\",  \"analyzer\": \"pinyin\"}结果：2.2.自定义分词器默认的拼音分词器会将每个汉字单独分为拼音，而我们希望的是每个词条形成一组拼音，需要对拼音分词器做个性化定制，形成自定义分词器。elasticsearch中分词器（analyzer）的组成包含三部分： character filters：在tokenizer之前对文本进行处理。例如删除字符、替换字符 tokenizer：将文本按照一定的规则切割成词条（term）。例如keyword，就是不分词；还有ik_smart tokenizer filter：将tokenizer输出的词条做进一步处理。例如大小写转换、同义词处理、拼音处理等文档分词时会依次由这三部分来处理文档：声明自定义分词器的语法如下：PUT /test{  \"settings\": {    \"analysis\": {      \"analyzer\": { // 自定义分词器        \"my_analyzer\": {  // 分词器名称          \"tokenizer\": \"ik_max_word\",          \"filter\": \"py\"        }      },      \"filter\": { // 自定义tokenizer filter        \"py\": { // 过滤器名称          \"type\": \"pinyin\", // 过滤器类型，这里是pinyin\t\t \"keep_full_pinyin\": false,          \"keep_joined_full_pinyin\": true,          \"keep_original\": true,          \"limit_first_letter_length\": 16,          \"remove_duplicated_term\": true,          \"none_chinese_pinyin_tokenize\": false        }      }    }  },  \"mappings\": {    \"properties\": {      \"name\": {        \"type\": \"text\",        \"analyzer\": \"my_analyzer\",        \"search_analyzer\": \"ik_smart\"      }    }  }}测试：总结：如何使用拼音分词器？ ①下载pinyin分词器 ②解压并放到elasticsearch的plugin目录 ③重启即可 如何自定义分词器？ ①创建索引库时，在settings中配置，可以包含三部分 ②character filter ③tokenizer ④filter 拼音分词器注意事项？ 为了避免搜索到同音字，搜索时不要使用拼音分词器2.3.自动补全查询elasticsearch提供了Completion Suggester查询来实现自动补全功能。这个查询会匹配以用户输入内容开头的词条并返回。为了提高补全查询的效率，对于文档中字段的类型有一些约束： 参与补全查询的字段必须是completion类型。 字段的内容一般是用来补全的多个词条形成的数组。 比如，一个这样的索引库：// 创建索引库PUT test{  \"mappings\": {    \"properties\": {      \"title\":{        \"type\": \"completion\"      }    }  }}然后插入下面的数据：// 示例数据POST test/_doc{  \"title\": [\"Sony\", \"WH-1000XM3\"]}POST test/_doc{  \"title\": [\"SK-II\", \"PITERA\"]}POST test/_doc{  \"title\": [\"Nintendo\", \"switch\"]}查询的DSL语句如下：// 自动补全查询GET /test/_search{  \"suggest\": {    \"title_suggest\": {      \"text\": \"s\", // 关键字      \"completion\": {        \"field\": \"title\", // 补全查询的字段        \"skip_duplicates\": true, // 跳过重复的        \"size\": 10 // 获取前10条结果      }    }  }}2.4.实现酒店搜索框自动补全现在，我们的hotel索引库还没有设置拼音分词器，需要修改索引库中的配置。但是我们知道索引库是无法修改的，只能删除然后重新创建。另外，我们需要添加一个字段，用来做自动补全，将brand、suggestion、city等都放进去，作为自动补全的提示。因此，总结一下，我们需要做的事情包括： 修改hotel索引库结构，设置自定义拼音分词器 修改索引库的name、all字段，使用自定义分词器 索引库添加一个新字段suggestion，类型为completion类型，使用自定义的分词器 给HotelDoc类添加suggestion字段，内容包含brand、business 重新导入数据到hotel库 2.4.1.修改酒店映射结构代码如下：// 酒店数据索引库PUT /hotel{ \"settings\": { \"analysis\": { \"analyzer\": { \"text_anlyzer\": { \"tokenizer\": \"ik_max_word\", \"filter\": \"py\" }, \"completion_analyzer\": { \"tokenizer\": \"keyword\", \"filter\": \"py\" } }, \"filter\": { \"py\": { \"type\": \"pinyin\", \"keep_full_pinyin\": false, \"keep_joined_full_pinyin\": true, \"keep_original\": true, \"limit_first_letter_length\": 16, \"remove_duplicated_term\": true, \"none_chinese_pinyin_tokenize\": false } } } }, \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\", \"analyzer\": \"text_anlyzer\", \"search_analyzer\": \"ik_smart\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\", \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\", \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"text_anlyzer\", \"search_analyzer\": \"ik_smart\" }, \"suggestion\":{ \"type\": \"completion\", \"analyzer\": \"completion_analyzer\" } } }}2.4.2.修改HotelDoc实体HotelDoc中要添加一个字段，用来做自动补全，内容可以是酒店品牌、城市、商圈等信息。按照自动补全字段的要求，最好是这些字段的数组。因此我们在HotelDoc中添加一个suggestion字段，类型为List&lt;String&gt;，然后将brand、city、business等信息放到里面。代码如下：package cn.itcast.hotel.pojo;import lombok.Data;import lombok.NoArgsConstructor;import java.util.ArrayList;import java.util.Arrays;import java.util.Collections;import java.util.List;@Data@NoArgsConstructorpublic class HotelDoc { private Long id; private String name; private String address; private Integer price; private Integer score; private String brand; private String city; private String starName; private String business; private String location; private String pic; private Object distance; private Boolean isAD; private List&lt;String&gt; suggestion; public HotelDoc(Hotel hotel) { this.id = hotel.getId(); this.name = hotel.getName(); this.address = hotel.getAddress(); this.price = hotel.getPrice(); this.score = hotel.getScore(); this.brand = hotel.getBrand(); this.city = hotel.getCity(); this.starName = hotel.getStarName(); this.business = hotel.getBusiness(); this.location = hotel.getLatitude() + \", \" + hotel.getLongitude(); this.pic = hotel.getPic(); // 组装suggestion if(this.business.contains(\"/\")){ // business有多个值，需要切割 String[] arr = this.business.split(\"/\"); // 添加元素 this.suggestion = new ArrayList&lt;&gt;(); this.suggestion.add(this.brand); Collections.addAll(this.suggestion, arr); }else { this.suggestion = Arrays.asList(this.brand, this.business); } }}2.4.3.重新导入重新执行之前编写的导入数据功能，可以看到新的酒店数据中包含了suggestion：2.4.4.自动补全查询的JavaAPI之前我们学习了自动补全查询的DSL，而没有学习对应的JavaAPI，这里给出一个示例：而自动补全的结果也比较特殊，解析的代码如下：2.4.5.实现搜索框自动补全查看前端页面，可以发现当我们在输入框键入时，前端会发起ajax请求：返回值是补全词条的集合，类型为List&lt;String&gt;1）在cn.itcast.hotel.web包下的HotelController中添加新接口，接收新的请求：@GetMapping(\"suggestion\")public List&lt;String&gt; getSuggestions(@RequestParam(\"key\") String prefix) { return hotelService.getSuggestions(prefix);}2）在cn.itcast.hotel.service包下的IhotelService中添加方法：List&lt;String&gt; getSuggestions(String prefix);3）在cn.itcast.hotel.service.impl.HotelService中实现该方法：@Overridepublic List&lt;String&gt; getSuggestions(String prefix) { try { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL request.source().suggest(new SuggestBuilder().addSuggestion( \"suggestions\", SuggestBuilders.completionSuggestion(\"suggestion\") .prefix(prefix) .skipDuplicates(true) .size(10) )); // 3.发起请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析结果 Suggest suggest = response.getSuggest(); // 4.1.根据补全查询名称，获取补全结果 CompletionSuggestion suggestions = suggest.getSuggestion(\"suggestions\"); // 4.2.获取options List&lt;CompletionSuggestion.Entry.Option&gt; options = suggestions.getOptions(); // 4.3.遍历 List&lt;String&gt; list = new ArrayList&lt;&gt;(options.size()); for (CompletionSuggestion.Entry.Option option : options) { String text = option.getText().toString(); list.add(text); } return list; } catch (IOException e) { throw new RuntimeException(e); }}3.数据同步elasticsearch中的酒店数据来自于mysql数据库，因此mysql数据发生改变时，elasticsearch也必须跟着改变，这个就是elasticsearch与mysql之间的数据同步。3.1.思路分析常见的数据同步方案有三种： 同步调用 异步通知 监听binlog3.1.1.同步调用方案一：同步调用基本步骤如下： hotel-demo对外提供接口，用来修改elasticsearch中的数据 酒店管理服务在完成数据库操作后，直接调用hotel-demo提供的接口，3.1.2.异步通知方案二：异步通知流程如下： hotel-admin对mysql数据库数据完成增、删、改后，发送MQ消息 hotel-demo监听MQ，接收到消息后完成elasticsearch数据修改3.1.3.监听binlog方案三：监听binlog流程如下： 给mysql开启binlog功能 mysql完成增、删、改操作都会记录在binlog中 hotel-demo基于canal监听binlog变化，实时更新elasticsearch中的内容3.1.4.选择方式一：同步调用 优点：实现简单，粗暴 缺点：业务耦合度高方式二：异步通知 优点：低耦合，实现难度一般 缺点：依赖mq的可靠性方式三：监听binlog 优点：完全解除服务间耦合 缺点：开启binlog增加数据库负担、实现复杂度高3.2.实现数据同步3.2.1.思路利用课前资料提供的hotel-admin项目作为酒店管理的微服务。当酒店数据发生增、删、改时，要求对elasticsearch中数据也要完成相同操作。步骤： 导入课前资料提供的hotel-admin项目，启动并测试酒店数据的CRUD 声明exchange、queue、RoutingKey 在hotel-admin中的增、删、改业务中完成消息发送 在hotel-demo中完成消息监听，并更新elasticsearch中数据 启动并测试数据同步功能 3.2.2.导入demo导入课前资料提供的hotel-admin项目：运行后，访问 http://localhost:8099其中包含了酒店的CRUD功能：3.2.3.声明交换机、队列MQ结构如图：1）引入依赖在hotel-admin、hotel-demo中引入rabbitmq的依赖：&lt;!--amqp--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;2）声明队列交换机名称在hotel-admin和hotel-demo中的cn.itcast.hotel.constatnts包下新建一个类MqConstants：package cn.itcast.hotel.constatnts; public class MqConstants { /** * 交换机 */ public final static String HOTEL_EXCHANGE = \"hotel.topic\"; /** * 监听新增和修改的队列 */ public final static String HOTEL_INSERT_QUEUE = \"hotel.insert.queue\"; /** * 监听删除的队列 */ public final static String HOTEL_DELETE_QUEUE = \"hotel.delete.queue\"; /** * 新增或修改的RoutingKey */ public final static String HOTEL_INSERT_KEY = \"hotel.insert\"; /** * 删除的RoutingKey */ public final static String HOTEL_DELETE_KEY = \"hotel.delete\";}3）声明队列交换机在hotel-demo中，定义配置类，声明队列、交换机：package cn.itcast.hotel.config;import cn.itcast.hotel.constants.MqConstants;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.Queue;import org.springframework.amqp.core.TopicExchange;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class MqConfig { @Bean public TopicExchange topicExchange(){ return new TopicExchange(MqConstants.HOTEL_EXCHANGE, true, false); } @Bean public Queue insertQueue(){ return new Queue(MqConstants.HOTEL_INSERT_QUEUE, true); } @Bean public Queue deleteQueue(){ return new Queue(MqConstants.HOTEL_DELETE_QUEUE, true); } @Bean public Binding insertQueueBinding(){ return BindingBuilder.bind(insertQueue()).to(topicExchange()).with(MqConstants.HOTEL_INSERT_KEY); } @Bean public Binding deleteQueueBinding(){ return BindingBuilder.bind(deleteQueue()).to(topicExchange()).with(MqConstants.HOTEL_DELETE_KEY); }}3.2.4.发送MQ消息在hotel-admin中的增、删、改业务中分别发送MQ消息：3.2.5.接收MQ消息hotel-demo接收到MQ消息要做的事情包括： 新增消息：根据传递的hotel的id查询hotel信息，然后新增一条数据到索引库 删除消息：根据传递的hotel的id删除索引库中的一条数据1）首先在hotel-demo的cn.itcast.hotel.service包下的IHotelService中新增新增、删除业务void deleteById(Long id);void insertById(Long id);2）给hotel-demo中的cn.itcast.hotel.service.impl包下的HotelService中实现业务：@Overridepublic void deleteById(Long id) { try { // 1.准备Request DeleteRequest request = new DeleteRequest(\"hotel\", id.toString()); // 2.发送请求 client.delete(request, RequestOptions.DEFAULT); } catch (IOException e) { throw new RuntimeException(e); }}@Overridepublic void insertById(Long id) { try { // 0.根据id查询酒店数据 Hotel hotel = getById(id); // 转换为文档类型 HotelDoc hotelDoc = new HotelDoc(hotel); // 1.准备Request对象 IndexRequest request = new IndexRequest(\"hotel\").id(hotel.getId().toString()); // 2.准备Json文档 request.source(JSON.toJSONString(hotelDoc), XContentType.JSON); // 3.发送请求 client.index(request, RequestOptions.DEFAULT); } catch (IOException e) { throw new RuntimeException(e); }}3）编写监听器在hotel-demo中的cn.itcast.hotel.mq包新增一个类：package cn.itcast.hotel.mq;import cn.itcast.hotel.constants.MqConstants;import cn.itcast.hotel.service.IHotelService;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;@Componentpublic class HotelListener { @Autowired private IHotelService hotelService; /** * 监听酒店新增或修改的业务 * @param id 酒店id */ @RabbitListener(queues = MqConstants.HOTEL_INSERT_QUEUE) public void listenHotelInsertOrUpdate(Long id){ hotelService.insertById(id); } /** * 监听酒店删除的业务 * @param id 酒店id */ @RabbitListener(queues = MqConstants.HOTEL_DELETE_QUEUE) public void listenHotelDelete(Long id){ hotelService.deleteById(id); }}4.集群单机的elasticsearch做数据存储，必然面临两个问题：海量数据存储问题、单点故障问题。 海量数据存储问题：将索引库从逻辑上拆分为N个分片（shard），存储到多个节点 单点故障问题：将分片数据在不同节点备份（replica ）ES集群相关概念: 集群（cluster）：一组拥有共同的 cluster name 的 节点。 节点（node) ：集群中的一个 Elasticearch 实例 分片（shard） ：索引可以被拆分为不同的部分进行存储，称为分片。在集群环境下，一个索引的不同分片可以拆分到不同的节点中 解决问题：数据量太大，单点存储量有限的问题。 此处，我们把数据分成3片：shard0、shard1、shard2 主分片（Primary shard）：相对于副本分片的定义。 副本分片（Replica shard）每个主分片可以有一个或者多个副本，数据和主分片一样。 ​ 数据备份可以保证高可用，但是每个分片备份一份，所需要的节点数量就会翻一倍，成本实在是太高了！为了在高可用和成本间寻求平衡，我们可以这样做： 首先对数据分片，存储到不同节点 然后对每个分片进行备份，放到对方节点，完成互相备份这样可以大大减少所需要的服务节点数量，如图，我们以3分片，每个分片备份一份为例：现在，每个分片都有1个备份，存储在3个节点： node0：保存了分片0和1 node1：保存了分片0和2 node2：保存了分片1和24.1.搭建ES集群参考课前资料的文档：其中的第四章节：4.2.集群脑裂问题4.2.1.集群职责划分elasticsearch中集群节点有不同的职责划分：默认情况下，集群中的任何一个节点都同时具备上述四种角色。但是真实的集群一定要将集群职责分离： master节点：对CPU要求高，但是内存要求第 data节点：对CPU和内存要求都高 coordinating节点：对网络带宽、CPU要求高职责分离可以让我们根据不同节点的需求分配不同的硬件去部署。而且避免业务之间的互相干扰。一个典型的es集群职责划分如图：4.2.2.脑裂问题脑裂是因为集群中的节点失联导致的。例如一个集群中，主节点与其它节点失联：此时，node2和node3认为node1宕机，就会重新选主：当node3当选后，集群继续对外提供服务，node2和node3自成集群，node1自成集群，两个集群数据不同步，出现数据差异。当网络恢复后，因为集群中有两个master节点，集群状态的不一致，出现脑裂的情况：解决脑裂的方案是，要求选票超过 ( eligible节点数量 + 1 ）/ 2 才能当选为主，因此eligible节点数量最好是奇数。对应配置项是discovery.zen.minimum_master_nodes，在es7.0以后，已经成为默认配置，因此一般不会发生脑裂问题例如：3个节点形成的集群，选票必须超过 （3 + 1） / 2 ，也就是2票。node3得到node2和node3的选票，当选为主。node1只有自己1票，没有当选。集群中依然只有1个主节点，没有出现脑裂。4.2.3.小结master eligible节点的作用是什么？ 参与集群选主 主节点可以管理集群状态、管理分片信息、处理创建和删除索引库的请求data节点的作用是什么？ 数据的CRUDcoordinator节点的作用是什么？ 路由请求到其它节点 合并查询到的结果，返回给用户 4.3.集群分布式存储当新增文档时，应该保存到不同分片，保证数据均衡，那么coordinating node如何确定数据该存储到哪个分片呢？4.3.1.分片存储测试插入三条数据：测试可以看到，三条数据分别在不同分片：结果：4.3.2.分片存储原理elasticsearch会通过hash算法来计算文档应该存储到哪个分片：说明： _routing默认是文档的id 算法与分片数量有关，因此索引库一旦创建，分片数量不能修改！新增文档的流程如下：解读： 1）新增一个id=1的文档 2）对id做hash运算，假如得到的是2，则应该存储到shard-2 3）shard-2的主分片在node3节点，将数据路由到node3 4）保存文档 5）同步给shard-2的副本replica-2，在node2节点 6）返回结果给coordinating-node节点4.4.集群分布式查询elasticsearch的查询分成两个阶段： scatter phase：分散阶段，coordinating node会把请求分发到每一个分片 gather phase：聚集阶段，coordinating node汇总data node的搜索结果，并处理为最终结果集返回给用户 4.5.集群故障转移集群的master节点会监控集群中的节点状态，如果发现有节点宕机，会立即将宕机节点的分片数据迁移到其它节点，确保数据安全，这个叫做故障转移。1）例如一个集群结构如图：现在，node1是主节点，其它两个节点是从节点。2）突然，node1发生了故障：宕机后的第一件事，需要重新选主，例如选中了node2：node2成为主节点后，会检测集群监控状态，发现：shard-1、shard-0没有副本节点。因此需要将node1上的数据迁移到node2、node3：" }, { "title": "分布式搜索引擎02", "url": "/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E02/", "categories": "微服务", "tags": "学习", "date": "2024-03-12 02:34:00 +0000", "snippet": "分布式搜索引擎02在昨天的学习中，我们已经导入了大量数据到elasticsearch中，实现了elasticsearch的数据存储功能。但elasticsearch最擅长的还是搜索和数据分析。所以今天，我们研究下elasticsearch的数据搜索功能。我们会分别使用DSL和RestClient实现搜索。0.学习目标1.DSL查询文档elasticsearch的查询依然是基于JSON风格的D...", "content": "分布式搜索引擎02在昨天的学习中，我们已经导入了大量数据到elasticsearch中，实现了elasticsearch的数据存储功能。但elasticsearch最擅长的还是搜索和数据分析。所以今天，我们研究下elasticsearch的数据搜索功能。我们会分别使用DSL和RestClient实现搜索。0.学习目标1.DSL查询文档elasticsearch的查询依然是基于JSON风格的DSL来实现的。1.1.DSL查询分类Elasticsearch提供了基于JSON的DSL（Domain Specific Language）来定义查询。常见的查询类型包括： 查询所有：查询出所有数据，一般测试用。例如：match_all 全文检索（full text）查询：利用分词器对用户输入内容分词，然后去倒排索引库中匹配。例如： match_query multi_match_query 精确查询：根据精确词条值查找数据，一般是查找keyword、数值、日期、boolean等类型字段。例如： ids range term 地理（geo）查询：根据经纬度查询。例如： geo_distance geo_bounding_box 复合（compound）查询：复合查询可以将上述各种查询条件组合起来，合并查询条件。例如： bool function_score 查询的语法基本一致：GET /indexName/_search{  \"query\": {    \"查询类型\": {      \"查询条件\": \"条件值\"    }  }}我们以查询所有为例，其中： 查询类型为match_all 没有查询条件// 查询所有GET /indexName/_search{  \"query\": {    \"match_all\": { }  }}其它查询无非就是查询类型、查询条件的变化。1.2.全文检索查询1.2.1.使用场景全文检索查询的基本流程如下： 对用户搜索的内容做分词，得到词条 根据词条去倒排索引库中匹配，得到文档id 根据文档id找到文档，返回给用户比较常用的场景包括： 商城的输入框搜索 百度输入框搜索例如京东：因为是拿着词条去匹配，因此参与搜索的字段也必须是可分词的text类型的字段。1.2.2.基本语法常见的全文检索查询包括： match查询：单字段查询 multi_match查询：多字段查询，任意一个字段符合条件就算符合查询条件match查询语法如下：GET /indexName/_search{  \"query\": {    \"match\": {      \"FIELD\": \"TEXT\"    }  }}mulit_match语法如下：GET /indexName/_search{  \"query\": {    \"multi_match\": {      \"query\": \"TEXT\",      \"fields\": [\"FIELD1\", \" FIELD12\"]    }  }}1.2.3.示例match查询示例：multi_match查询示例：可以看到，两种查询结果是一样的，为什么？因为我们将brand、name、business值都利用copy_to复制到了all字段中。因此你根据三个字段搜索，和根据all字段搜索效果当然一样了。但是，搜索字段越多，对查询性能影响越大，因此建议采用copy_to，然后单字段查询的方式。1.2.4.总结match和multi_match的区别是什么？ match：根据一个字段查询 multi_match：根据多个字段查询，参与查询字段越多，查询性能越差1.3.精准查询精确查询一般是查找keyword、数值、日期、boolean等类型字段。所以不会对搜索条件分词。常见的有： term：根据词条精确值查询 range：根据值的范围查询1.3.1.term查询因为精确查询的字段搜是不分词的字段，因此查询的条件也必须是不分词的词条。查询时，用户输入的内容跟自动值完全匹配时才认为符合条件。如果用户输入的内容过多，反而搜索不到数据。语法说明：// term查询GET /indexName/_search{  \"query\": {    \"term\": {      \"FIELD\": {        \"value\": \"VALUE\"      }    }  }}示例：当我搜索的是精确词条时，能正确查询出结果：但是，当我搜索的内容不是词条，而是多个词语形成的短语时，反而搜索不到：1.3.2.range查询范围查询，一般应用在对数值类型做范围过滤的时候。比如做价格范围过滤。基本语法：// range查询GET /indexName/_search{  \"query\": {    \"range\": {      \"FIELD\": {        \"gte\": 10, // 这里的gte代表大于等于，gt则代表大于        \"lte\": 20 // lte代表小于等于，lt则代表小于      }    }  }}示例：1.3.3.总结精确查询常见的有哪些？ term查询：根据词条精确匹配，一般搜索keyword类型、数值类型、布尔类型、日期类型字段 range查询：根据数值范围查询，可以是数值、日期的范围1.4.地理坐标查询所谓的地理坐标查询，其实就是根据经纬度查询，官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html常见的使用场景包括： 携程：搜索我附近的酒店 滴滴：搜索我附近的出租车 微信：搜索我附近的人附近的酒店：附近的车：1.4.1.矩形范围查询矩形范围查询，也就是geo_bounding_box查询，查询坐标落在某个矩形范围的所有文档：查询时，需要指定矩形的左上、右下两个点的坐标，然后画出一个矩形，落在该矩形内的都是符合条件的点。语法如下：// geo_bounding_box查询GET /indexName/_search{  \"query\": {    \"geo_bounding_box\": {      \"FIELD\": {        \"top_left\": { // 左上点          \"lat\": 31.1,          \"lon\": 121.5        },        \"bottom_right\": { // 右下点          \"lat\": 30.9,          \"lon\": 121.7        }      }    }  }}这种并不符合“附近的人”这样的需求，所以我们就不做了。1.4.2.附近查询附近查询，也叫做距离查询（geo_distance）：查询到指定中心点小于某个距离值的所有文档。换句话来说，在地图上找一个点作为圆心，以指定距离为半径，画一个圆，落在圆内的坐标都算符合条件：语法说明：// geo_distance 查询GET /indexName/_search{  \"query\": {    \"geo_distance\": {      \"distance\": \"15km\", // 半径      \"FIELD\": \"31.21,121.5\" // 圆心    }  }}示例：我们先搜索陆家嘴附近15km的酒店：发现共有47家酒店。然后把半径缩短到3公里：可以发现，搜索到的酒店数量减少到了5家。1.5.复合查询复合（compound）查询：复合查询可以将其它简单查询组合起来，实现更复杂的搜索逻辑。常见的有两种： fuction score：算分函数查询，可以控制文档相关性算分，控制文档排名 bool query：布尔查询，利用逻辑关系组合多个其它的查询，实现复杂搜索1.5.1.相关性算分当我们利用match查询时，文档结果会根据与搜索词条的关联度打分（_score），返回结果时按照分值降序排列。例如，我们搜索 “虹桥如家”，结果如下：[  {    \"_score\" : 17.850193,    \"_source\" : {      \"name\" : \"虹桥如家酒店真不错\",    }  },  {    \"_score\" : 12.259849,    \"_source\" : {      \"name\" : \"外滩如家酒店真不错\",    }  },  {    \"_score\" : 11.91091,    \"_source\" : {      \"name\" : \"迪士尼如家酒店真不错\",    }  }]在elasticsearch中，早期使用的打分算法是TF-IDF算法，公式如下：在后来的5.1版本升级中，elasticsearch将算法改进为BM25算法，公式如下：TF-IDF算法有一各缺陷，就是词条频率越高，文档得分也会越高，单个词条对文档影响较大。而BM25则会让单个词条的算分有一个上限，曲线更加平滑：小结：elasticsearch会根据词条和文档的相关度做打分，算法由两种： TF-IDF算法 BM25算法，elasticsearch5.1版本后采用的算法1.5.2.算分函数查询根据相关度打分是比较合理的需求，但合理的不一定是产品经理需要的。以百度为例，你搜索的结果中，并不是相关度越高排名越靠前，而是谁掏的钱多排名就越靠前。如图：要想认为控制相关性算分，就需要利用elasticsearch中的function score 查询了。1）语法说明function score 查询中包含四部分内容： 原始查询条件：query部分，基于这个条件搜索文档，并且基于BM25算法给文档打分，原始算分（query score) 过滤条件：filter部分，符合该条件的文档才会重新算分 算分函数：符合filter条件的文档要根据这个函数做运算，得到的函数算分（function score），有四种函数 weight：函数结果是常量 field_value_factor：以文档中的某个字段值作为函数结果 random_score：以随机数作为函数结果 script_score：自定义算分函数算法 运算模式：算分函数的结果、原始查询的相关性算分，两者之间的运算方式，包括： multiply：相乘 replace：用function score替换query score 其它，例如：sum、avg、max、min function score的运行流程如下： 1）根据原始条件查询搜索文档，并且计算相关性算分，称为原始算分（query score） 2）根据过滤条件，过滤文档 3）符合过滤条件的文档，基于算分函数运算，得到函数算分（function score） 4）将原始算分（query score）和函数算分（function score）基于运算模式做运算，得到最终结果，作为相关性算分。因此，其中的关键点是： 过滤条件：决定哪些文档的算分被修改 算分函数：决定函数算分的算法 运算模式：决定最终算分结果2）示例需求：给“如家”这个品牌的酒店排名靠前一些翻译一下这个需求，转换为之前说的四个要点： 原始条件：不确定，可以任意变化 过滤条件：brand = “如家” 算分函数：可以简单粗暴，直接给固定的算分结果，weight 运算模式：比如求和因此最终的DSL语句如下：GET /hotel/_search{  \"query\": {    \"function_score\": {      \"query\": { .... }, // 原始查询，可以是任意条件      \"functions\": [ // 算分函数        {          \"filter\": { // 满足的条件，品牌必须是如家            \"term\": {              \"brand\": \"如家\"            }          },          \"weight\": 2 // 算分权重为2        }      ], \"boost_mode\": \"sum\" // 加权模式，求和    }  }}测试，在未添加算分函数时，如家得分如下：添加了算分函数后，如家得分就提升了：3）小结function score query定义的三要素是什么？ 过滤条件：哪些文档要加分 算分函数：如何计算function score 加权方式：function score 与 query score如何运算1.5.3.布尔查询布尔查询是一个或多个查询子句的组合，每一个子句就是一个子查询。子查询的组合方式有： must：必须匹配每个子查询，类似“与” should：选择性匹配子查询，类似“或” must_not：必须不匹配，不参与算分，类似“非” filter：必须匹配，不参与算分比如在搜索酒店时，除了关键字搜索外，我们还可能根据品牌、价格、城市等字段做过滤：每一个不同的字段，其查询的条件、方式都不一样，必须是多个不同的查询，而要组合这些查询，就必须用bool查询了。需要注意的是，搜索时，参与打分的字段越多，查询的性能也越差。因此这种多条件查询时，建议这样做： 搜索框的关键字搜索，是全文检索查询，使用must查询，参与算分 其它过滤条件，采用filter查询。不参与算分1）语法示例：GET /hotel/_search{  \"query\": {    \"bool\": {      \"must\": [        {\"term\": {\"city\": \"上海\" }}      ],      \"should\": [        {\"term\": {\"brand\": \"皇冠假日\" }}, {\"term\": {\"brand\": \"华美达\" }}      ],      \"must_not\": [        { \"range\": { \"price\": { \"lte\": 500 } }}      ],      \"filter\": [        { \"range\": {\"score\": { \"gte\": 45 } }}      ]    }  }}2）示例需求：搜索名字包含“如家”，价格不高于400，在坐标31.21,121.5周围10km范围内的酒店。分析： 名称搜索，属于全文检索查询，应该参与算分。放到must中 价格不高于400，用range查询，属于过滤条件，不参与算分。放到must_not中 周围10km范围内，用geo_distance查询，属于过滤条件，不参与算分。放到filter中3）小结bool查询有几种逻辑关系？ must：必须匹配的条件，可以理解为“与” should：选择性匹配的条件，可以理解为“或” must_not：必须不匹配的条件，不参与打分 filter：必须匹配的条件，不参与打分2.搜索结果处理搜索的结果可以按照用户指定的方式去处理或展示。2.1.排序elasticsearch默认是根据相关度算分（_score）来排序，但是也支持自定义方式对搜索结果排序。可以排序字段类型有：keyword类型、数值类型、地理坐标类型、日期类型等。2.1.1.普通字段排序keyword、数值、日期类型排序的语法基本一致。语法：GET /indexName/_search{  \"query\": {    \"match_all\": {}  },  \"sort\": [    {      \"FIELD\": \"desc\"  // 排序字段、排序方式ASC、DESC    }  ]}排序条件是一个数组，也就是可以写多个排序条件。按照声明的顺序，当第一个条件相等时，再按照第二个条件排序，以此类推示例：需求描述：酒店数据按照用户评价（score)降序排序，评价相同的按照价格(price)升序排序2.1.2.地理坐标排序地理坐标排序略有不同。语法说明：GET /indexName/_search{  \"query\": {    \"match_all\": {}  },  \"sort\": [    {      \"_geo_distance\" : {          \"FIELD\" : \"纬度，经度\", // 文档中geo_point类型的字段名、目标坐标点          \"order\" : \"asc\", // 排序方式          \"unit\" : \"km\" // 排序的距离单位      }    }  ]}这个查询的含义是： 指定一个坐标，作为目标点 计算每一个文档中，指定字段（必须是geo_point类型）的坐标 到目标点的距离是多少 根据距离排序示例：需求描述：实现对酒店数据按照到你的位置坐标的距离升序排序提示：获取你的位置的经纬度的方式：https://lbs.amap.com/demo/jsapi-v2/example/map/click-to-get-lnglat/假设我的位置是：31.034661，121.612282，寻找我周围距离最近的酒店。2.2.分页elasticsearch 默认情况下只返回top10的数据。而如果要查询更多数据就需要修改分页参数了。elasticsearch中通过修改from、size参数来控制要返回的分页结果： from：从第几个文档开始 size：总共查询几个文档类似于mysql中的limit ?, ?2.2.1.基本的分页分页的基本语法如下：GET /hotel/_search{  \"query\": {    \"match_all\": {}  },  \"from\": 0, // 分页开始的位置，默认为0  \"size\": 10, // 期望获取的文档总数  \"sort\": [    {\"price\": \"asc\"}  ]}2.2.2.深度分页问题现在，我要查询990~1000的数据，查询逻辑要这么写：GET /hotel/_search{  \"query\": {    \"match_all\": {}  },  \"from\": 990, // 分页开始的位置，默认为0  \"size\": 10, // 期望获取的文档总数  \"sort\": [    {\"price\": \"asc\"}  ]}这里是查询990开始的数据，也就是 第990~第1000条 数据。不过，elasticsearch内部分页时，必须先查询 0~1000条，然后截取其中的990 ~ 1000的这10条：查询TOP1000，如果es是单点模式，这并无太大影响。但是elasticsearch将来一定是集群，例如我集群有5个节点，我要查询TOP1000的数据，并不是每个节点查询200条就可以了。因为节点A的TOP200，在另一个节点可能排到10000名以外了。因此要想获取整个集群的TOP1000，必须先查询出每个节点的TOP1000，汇总结果后，重新排名，重新截取TOP1000。那如果我要查询9900~10000的数据呢？是不是要先查询TOP10000呢？那每个节点都要查询10000条？汇总到内存中？当查询分页深度较大时，汇总数据过多，对内存和CPU会产生非常大的压力，因此elasticsearch会禁止from+ size 超过10000的请求。针对深度分页，ES提供了两种解决方案，官方文档： search after：分页时需要排序，原理是从上一次的排序值开始，查询下一页数据。官方推荐使用的方式。 scroll：原理将排序后的文档id形成快照，保存在内存。官方已经不推荐使用。2.2.3.小结分页查询的常见实现方案以及优缺点： from + size： 优点：支持随机翻页 缺点：深度分页问题，默认查询上限（from + size）是10000 场景：百度、京东、谷歌、淘宝这样的随机翻页搜索 after search： 优点：没有查询上限（单次查询的size不超过10000） 缺点：只能向后逐页查询，不支持随机翻页 场景：没有随机翻页需求的搜索，例如手机向下滚动翻页 scroll： 优点：没有查询上限（单次查询的size不超过10000） 缺点：会有额外内存消耗，并且搜索结果是非实时的 场景：海量数据的获取和迁移。从ES7.1开始不推荐，建议用 after search方案。 2.3.高亮2.3.1.高亮原理什么是高亮显示呢？我们在百度，京东搜索时，关键字会变成红色，比较醒目，这叫高亮显示：高亮显示的实现分为两步： 1）给文档中的所有关键字都添加一个标签，例如&lt;em&gt;标签 2）页面给&lt;em&gt;标签编写CSS样式2.3.2.实现高亮高亮的语法：GET /hotel/_search{  \"query\": {    \"match\": {      \"FIELD\": \"TEXT\" // 查询条件，高亮一定要使用全文检索查询    }  },  \"highlight\": {    \"fields\": { // 指定要高亮的字段      \"FIELD\": {        \"pre_tags\": \"&lt;em&gt;\",  // 用来标记高亮字段的前置标签        \"post_tags\": \"&lt;/em&gt;\" // 用来标记高亮字段的后置标签      }    }  }}注意： 高亮是对关键字高亮，因此搜索条件必须带有关键字，而不能是范围这样的查询。 默认情况下，高亮的字段，必须与搜索指定的字段一致，否则无法高亮 如果要对非搜索字段高亮，则需要添加一个属性：required_field_match=false示例：2.4.总结查询的DSL是一个大的JSON对象，包含下列属性： query：查询条件 from和size：分页条件 sort：排序条件 highlight：高亮条件示例：3.RestClient查询文档文档的查询同样适用昨天学习的 RestHighLevelClient对象，基本步骤包括： 1）准备Request对象 2）准备请求参数 3）发起请求 4）解析响应3.1.快速入门我们以match_all查询为例3.1.1.发起查询请求代码解读： 第一步，创建SearchRequest对象，指定索引库名 第二步，利用request.source()构建DSL，DSL中可以包含查询、分页、排序、高亮等 query()：代表查询条件，利用QueryBuilders.matchAllQuery()构建一个match_all查询的DSL 第三步，利用client.search()发送请求，得到响应这里关键的API有两个，一个是request.source()，其中包含了查询、排序、分页、高亮等所有功能：另一个是QueryBuilders，其中包含match、term、function_score、bool等各种查询：3.1.2.解析响应响应结果的解析：elasticsearch返回的结果是一个JSON字符串，结构包含： hits：命中的结果 total：总条数，其中的value是具体的总条数值 max_score：所有结果中得分最高的文档的相关性算分 hits：搜索结果的文档数组，其中的每个文档都是一个json对象 _source：文档中的原始数据，也是json对象 因此，我们解析响应结果，就是逐层解析JSON字符串，流程如下： SearchHits：通过response.getHits()获取，就是JSON中的最外层的hits，代表命中的结果 SearchHits#getTotalHits().value：获取总条数信息 SearchHits#getHits()：获取SearchHit数组，也就是文档数组 SearchHit#getSourceAsString()：获取文档结果中的_source，也就是原始的json文档数据 3.1.3.完整代码完整代码如下：@Testvoid testMatchAll() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL request.source() .query(QueryBuilders.matchAllQuery()); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);}private void handleResponse(SearchResponse response) { // 4.解析响应 SearchHits searchHits = response.getHits(); // 4.1.获取总条数 long total = searchHits.getTotalHits().value; System.out.println(\"共搜索到\" + total + \"条数据\"); // 4.2.文档数组 SearchHit[] hits = searchHits.getHits(); // 4.3.遍历 for (SearchHit hit : hits) { // 获取文档source String json = hit.getSourceAsString(); // 反序列化 HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class); System.out.println(\"hotelDoc = \" + hotelDoc); }}3.1.4.小结查询的基本步骤是： 创建SearchRequest对象 准备Request.source()，也就是DSL。 ① QueryBuilders来构建查询条件 ② 传入Request.source() 的 query() 方法 发送请求，得到结果 解析结果（参考JSON结果，从外到内，逐层解析） 3.2.match查询全文检索的match和multi_match查询与match_all的API基本一致。差别是查询条件，也就是query的部分。因此，Java代码上的差异主要是request.source().query()中的参数了。同样是利用QueryBuilders提供的方法：而结果解析代码则完全一致，可以抽取并共享。完整代码如下：@Testvoid testMatch() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL request.source() .query(QueryBuilders.matchQuery(\"all\", \"如家\")); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);}3.3.精确查询精确查询主要是两者： term：词条精确匹配 range：范围查询与之前的查询相比，差异同样在查询条件，其它都一样。查询条件构造的API如下：3.4.布尔查询布尔查询是用must、must_not、filter等方式组合其它查询，代码示例如下：可以看到，API与其它查询的差别同样是在查询条件的构建，QueryBuilders，结果解析等其他代码完全不变。完整代码如下：@Testvoid testBool() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.准备BooleanQuery BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 2.2.添加term boolQuery.must(QueryBuilders.termQuery(\"city\", \"杭州\")); // 2.3.添加range boolQuery.filter(QueryBuilders.rangeQuery(\"price\").lte(250)); request.source().query(boolQuery); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);}3.5.排序、分页搜索结果的排序和分页是与query同级的参数，因此同样是使用request.source()来设置。对应的API如下：完整代码示例：@Testvoid testPageAndSort() throws IOException { // 页码，每页大小 int page = 1, size = 5; // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query request.source().query(QueryBuilders.matchAllQuery()); // 2.2.排序 sort request.source().sort(\"price\", SortOrder.ASC); // 2.3.分页 from、size request.source().from((page - 1) * size).size(5); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);}3.6.高亮高亮的代码与之前代码差异较大，有两点： 查询的DSL：其中除了查询条件，还需要添加高亮条件，同样是与query同级。 结果解析：结果除了要解析_source文档数据，还要解析高亮结果3.6.1.高亮请求构建高亮请求的构建API如下：上述代码省略了查询条件部分，但是大家不要忘了：高亮查询必须使用全文检索查询，并且要有搜索关键字，将来才可以对关键字高亮。完整代码如下：@Testvoid testHighlight() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query request.source().query(QueryBuilders.matchQuery(\"all\", \"如家\")); // 2.2.高亮 request.source().highlighter(new HighlightBuilder().field(\"name\").requireFieldMatch(false)); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);}3.6.2.高亮结果解析高亮的结果与查询的文档结果默认是分离的，并不在一起。因此解析高亮的代码需要额外处理：代码解读： 第一步：从结果中获取source。hit.getSourceAsString()，这部分是非高亮结果，json字符串。还需要反序列为HotelDoc对象 第二步：获取高亮结果。hit.getHighlightFields()，返回值是一个Map，key是高亮字段名称，值是HighlightField对象，代表高亮值 第三步：从map中根据高亮字段名称，获取高亮字段值对象HighlightField 第四步：从HighlightField中获取Fragments，并且转为字符串。这部分就是真正的高亮字符串了 第五步：用高亮的结果替换HotelDoc中的非高亮结果完整代码如下：private void handleResponse(SearchResponse response) { // 4.解析响应 SearchHits searchHits = response.getHits(); // 4.1.获取总条数 long total = searchHits.getTotalHits().value; System.out.println(\"共搜索到\" + total + \"条数据\"); // 4.2.文档数组 SearchHit[] hits = searchHits.getHits(); // 4.3.遍历 for (SearchHit hit : hits) { // 获取文档source String json = hit.getSourceAsString(); // 反序列化 HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class); // 获取高亮结果 Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields(); if (!CollectionUtils.isEmpty(highlightFields)) { // 根据字段名获取高亮结果 HighlightField highlightField = highlightFields.get(\"name\"); if (highlightField != null) { // 获取高亮值 String name = highlightField.getFragments()[0].string(); // 覆盖非高亮结果 hotelDoc.setName(name); } } System.out.println(\"hotelDoc = \" + hotelDoc); }}4.黑马旅游案例下面，我们通过黑马旅游的案例来实战演练下之前学习的知识。我们实现四部分功能： 酒店搜索和分页 酒店结果过滤 我周边的酒店 酒店竞价排名启动我们提供的hotel-demo项目，其默认端口是8089，访问http://localhost:8090，就能看到项目页面了：4.1.酒店搜索和分页案例需求：实现黑马旅游的酒店搜索功能，完成关键字搜索和分页4.1.1.需求分析在项目的首页，有一个大大的搜索框，还有分页按钮：点击搜索按钮，可以看到浏览器控制台发出了请求：请求参数如下：由此可以知道，我们这个请求的信息如下： 请求方式：POST 请求路径：/hotel/list 请求参数：JSON对象，包含4个字段： key：搜索关键字 page：页码 size：每页大小 sortBy：排序，目前暂不实现 返回值：分页查询，需要返回分页结果PageResult，包含两个属性： total：总条数 List&lt;HotelDoc&gt;：当前页的数据 因此，我们实现业务的流程如下： 步骤一：定义实体类，接收请求参数的JSON对象 步骤二：编写controller，接收页面的请求 步骤三：编写业务实现，利用RestHighLevelClient实现搜索、分页4.1.2.定义实体类实体类有两个，一个是前端的请求参数实体，一个是服务端应该返回的响应结果实体。1）请求参数前端请求的json结构如下：{ \"key\": \"搜索关键字\", \"page\": 1, \"size\": 3, \"sortBy\": \"default\"}因此，我们在cn.itcast.hotel.pojo包下定义一个实体类：package cn.itcast.hotel.pojo;import lombok.Data;@Datapublic class RequestParams { private String key; private Integer page; private Integer size; private String sortBy;}2）返回值分页查询，需要返回分页结果PageResult，包含两个属性： total：总条数 List&lt;HotelDoc&gt;：当前页的数据因此，我们在cn.itcast.hotel.pojo中定义返回结果：package cn.itcast.hotel.pojo;import lombok.Data;import java.util.List;@Datapublic class PageResult { private Long total; private List&lt;HotelDoc&gt; hotels; public PageResult() { } public PageResult(Long total, List&lt;HotelDoc&gt; hotels) { this.total = total; this.hotels = hotels; }}4.1.3.定义controller定义一个HotelController，声明查询接口，满足下列要求： 请求方式：Post 请求路径：/hotel/list 请求参数：对象，类型为RequestParam 返回值：PageResult，包含两个属性 Long total：总条数 List&lt;HotelDoc&gt; hotels：酒店数据 因此，我们在cn.itcast.hotel.web中定义HotelController：@RestController@RequestMapping(\"/hotel\")public class HotelController { @Autowired private IHotelService hotelService;\t// 搜索酒店数据 @PostMapping(\"/list\") public PageResult search(@RequestBody RequestParams params){ return hotelService.search(params); }}4.1.4.实现搜索业务我们在controller调用了IHotelService，并没有实现该方法，因此下面我们就在IHotelService中定义方法，并且去实现业务逻辑。1）在cn.itcast.hotel.service中的IHotelService接口中定义一个方法：/** * 根据关键字搜索酒店信息 * @param params 请求参数对象，包含用户输入的关键字 * @return 酒店文档列表 */PageResult search(RequestParams params);2）实现搜索业务，肯定离不开RestHighLevelClient，我们需要把它注册到Spring中作为一个Bean。在cn.itcast.hotel中的HotelDemoApplication中声明这个Bean：@Beanpublic RestHighLevelClient client(){ return new RestHighLevelClient(RestClient.builder( HttpHost.create(\"http://192.168.150.101:9200\") ));}3）在cn.itcast.hotel.service.impl中的HotelService中实现search方法：@Overridepublic PageResult search(RequestParams params) { try { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query String key = params.getKey(); if (key == null || \"\".equals(key)) { boolQuery.must(QueryBuilders.matchAllQuery()); } else { boolQuery.must(QueryBuilders.matchQuery(\"all\", key)); } // 2.2.分页 int page = params.getPage(); int size = params.getSize(); request.source().from((page - 1) * size).size(size); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 return handleResponse(response); } catch (IOException e) { throw new RuntimeException(e); }}// 结果解析private PageResult handleResponse(SearchResponse response) { // 4.解析响应 SearchHits searchHits = response.getHits(); // 4.1.获取总条数 long total = searchHits.getTotalHits().value; // 4.2.文档数组 SearchHit[] hits = searchHits.getHits(); // 4.3.遍历 List&lt;HotelDoc&gt; hotels = new ArrayList&lt;&gt;(); for (SearchHit hit : hits) { // 获取文档source String json = hit.getSourceAsString(); // 反序列化 HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);\t\t// 放入集合 hotels.add(hotelDoc); } // 4.4.封装返回 return new PageResult(total, hotels);}4.2.酒店结果过滤需求：添加品牌、城市、星级、价格等过滤功能4.2.1.需求分析在页面搜索框下面，会有一些过滤项：传递的参数如图：包含的过滤条件有： brand：品牌值 city：城市 minPrice~maxPrice：价格范围 starName：星级我们需要做两件事情： 修改请求参数的对象RequestParams，接收上述参数 修改业务逻辑，在搜索条件之外，添加一些过滤条件4.2.2.修改实体类修改在cn.itcast.hotel.pojo包下的实体类RequestParams：@Datapublic class RequestParams { private String key; private Integer page; private Integer size; private String sortBy; // 下面是新增的过滤条件参数 private String city; private String brand; private String starName; private Integer minPrice; private Integer maxPrice;}4.2.3.修改搜索业务在HotelService的search方法中，只有一个地方需要修改：requet.source().query( … )其中的查询条件。在之前的业务中，只有match查询，根据关键字搜索，现在要添加条件过滤，包括： 品牌过滤：是keyword类型，用term查询 星级过滤：是keyword类型，用term查询 价格过滤：是数值类型，用range查询 城市过滤：是keyword类型，用term查询多个查询条件组合，肯定是boolean查询来组合： 关键字搜索放到must中，参与算分 其它过滤条件放到filter中，不参与算分因为条件构建的逻辑比较复杂，这里先封装为一个函数：buildBasicQuery的代码如下：private void buildBasicQuery(RequestParams params, SearchRequest request) { // 1.构建BooleanQuery BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 2.关键字搜索 String key = params.getKey(); if (key == null || \"\".equals(key)) { boolQuery.must(QueryBuilders.matchAllQuery()); } else { boolQuery.must(QueryBuilders.matchQuery(\"all\", key)); } // 3.城市条件 if (params.getCity() != null &amp;&amp; !params.getCity().equals(\"\")) { boolQuery.filter(QueryBuilders.termQuery(\"city\", params.getCity())); } // 4.品牌条件 if (params.getBrand() != null &amp;&amp; !params.getBrand().equals(\"\")) { boolQuery.filter(QueryBuilders.termQuery(\"brand\", params.getBrand())); } // 5.星级条件 if (params.getStarName() != null &amp;&amp; !params.getStarName().equals(\"\")) { boolQuery.filter(QueryBuilders.termQuery(\"starName\", params.getStarName())); }\t// 6.价格 if (params.getMinPrice() != null &amp;&amp; params.getMaxPrice() != null) { boolQuery.filter(QueryBuilders .rangeQuery(\"price\") .gte(params.getMinPrice()) .lte(params.getMaxPrice()) ); }\t// 7.放入source request.source().query(boolQuery);}4.3.我周边的酒店需求：我附近的酒店4.3.1.需求分析在酒店列表页的右侧，有一个小地图，点击地图的定位按钮，地图会找到你所在的位置：并且，在前端会发起查询请求，将你的坐标发送到服务端：我们要做的事情就是基于这个location坐标，然后按照距离对周围酒店排序。实现思路如下： 修改RequestParams参数，接收location字段 修改search方法业务逻辑，如果location有值，添加根据geo_distance排序的功能4.3.2.修改实体类修改在cn.itcast.hotel.pojo包下的实体类RequestParams：package cn.itcast.hotel.pojo;import lombok.Data;@Datapublic class RequestParams { private String key; private Integer page; private Integer size; private String sortBy; private String city; private String brand; private String starName; private Integer minPrice; private Integer maxPrice; // 我当前的地理坐标 private String location;}4.3.3.距离排序API我们以前学习过排序功能，包括两种： 普通字段排序 地理坐标排序我们只讲了普通字段排序对应的java写法。地理坐标排序只学过DSL语法，如下：GET /indexName/_search{  \"query\": {    \"match_all\": {}  },  \"sort\": [    {      \"price\": \"asc\"      },    {      \"_geo_distance\" : {          \"FIELD\" : \"纬度，经度\",          \"order\" : \"asc\",          \"unit\" : \"km\"      }    }  ]}对应的java代码示例：4.3.4.添加距离排序在cn.itcast.hotel.service.impl的HotelService的search方法中，添加一个排序功能：完整代码：@Overridepublic PageResult search(RequestParams params) { try { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query buildBasicQuery(params, request); // 2.2.分页 int page = params.getPage(); int size = params.getSize(); request.source().from((page - 1) * size).size(size); // 2.3.排序 String location = params.getLocation(); if (location != null &amp;&amp; !location.equals(\"\")) { request.source().sort(SortBuilders .geoDistanceSort(\"location\", new GeoPoint(location)) .order(SortOrder.ASC) .unit(DistanceUnit.KILOMETERS) ); } // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 return handleResponse(response); } catch (IOException e) { throw new RuntimeException(e); }}4.3.5.排序距离显示重启服务后，测试我的酒店功能：发现确实可以实现对我附近酒店的排序，不过并没有看到酒店到底距离我多远，这该怎么办？排序完成后，页面还要获取我附近每个酒店的具体距离值，这个值在响应结果中是独立的：因此，我们在结果解析阶段，除了解析source部分以外，还要得到sort部分，也就是排序的距离，然后放到响应结果中。我们要做两件事： 修改HotelDoc，添加排序距离字段，用于页面显示 修改HotelService类中的handleResponse方法，添加对sort值的获取1）修改HotelDoc类，添加距离字段package cn.itcast.hotel.pojo;import lombok.Data;import lombok.NoArgsConstructor;@Data@NoArgsConstructorpublic class HotelDoc { private Long id; private String name; private String address; private Integer price; private Integer score; private String brand; private String city; private String starName; private String business; private String location; private String pic; // 排序时的 距离值 private Object distance; public HotelDoc(Hotel hotel) { this.id = hotel.getId(); this.name = hotel.getName(); this.address = hotel.getAddress(); this.price = hotel.getPrice(); this.score = hotel.getScore(); this.brand = hotel.getBrand(); this.city = hotel.getCity(); this.starName = hotel.getStarName(); this.business = hotel.getBusiness(); this.location = hotel.getLatitude() + \", \" + hotel.getLongitude(); this.pic = hotel.getPic(); }}2）修改HotelService中的handleResponse方法重启后测试，发现页面能成功显示距离了：4.4.酒店竞价排名需求：让指定的酒店在搜索结果中排名置顶4.4.1.需求分析要让指定酒店在搜索结果中排名置顶，效果如图：页面会给指定的酒店添加广告标记。那怎样才能让指定的酒店排名置顶呢？我们之前学习过的function_score查询可以影响算分，算分高了，自然排名也就高了。而function_score包含3个要素： 过滤条件：哪些文档要加分 算分函数：如何计算function score 加权方式：function score 与 query score如何运算这里的需求是：让指定酒店排名靠前。因此我们需要给这些酒店添加一个标记，这样在过滤条件中就可以根据这个标记来判断，是否要提高算分。比如，我们给酒店添加一个字段：isAD，Boolean类型： true：是广告 false：不是广告这样function_score包含3个要素就很好确定了： 过滤条件：判断isAD 是否为true 算分函数：我们可以用最简单暴力的weight，固定加权值 加权方式：可以用默认的相乘，大大提高算分因此，业务的实现步骤包括： 给HotelDoc类添加isAD字段，Boolean类型 挑选几个你喜欢的酒店，给它的文档数据添加isAD字段，值为true 修改search方法，添加function score功能，给isAD值为true的酒店增加权重 4.4.2.修改HotelDoc实体给cn.itcast.hotel.pojo包下的HotelDoc类添加isAD字段：4.4.3.添加广告标记接下来，我们挑几个酒店，添加isAD字段，设置为true：POST /hotel/_update/1902197537{ \"doc\": { \"isAD\": true }}POST /hotel/_update/2056126831{ \"doc\": { \"isAD\": true }}POST /hotel/_update/1989806195{ \"doc\": { \"isAD\": true }}POST /hotel/_update/2056105938{ \"doc\": { \"isAD\": true }}4.4.4.添加算分函数查询接下来我们就要修改查询条件了。之前是用的boolean 查询，现在要改成function_socre查询。function_score查询结构如下：对应的JavaAPI如下：我们可以将之前写的boolean查询作为原始查询条件放到query中，接下来就是添加过滤条件、算分函数、加权模式了。所以原来的代码依然可以沿用。修改cn.itcast.hotel.service.impl包下的HotelService类中的buildBasicQuery方法，添加算分函数查询：private void buildBasicQuery(RequestParams params, SearchRequest request) { // 1.构建BooleanQuery BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 关键字搜索 String key = params.getKey(); if (key == null || \"\".equals(key)) { boolQuery.must(QueryBuilders.matchAllQuery()); } else { boolQuery.must(QueryBuilders.matchQuery(\"all\", key)); } // 城市条件 if (params.getCity() != null &amp;&amp; !params.getCity().equals(\"\")) { boolQuery.filter(QueryBuilders.termQuery(\"city\", params.getCity())); } // 品牌条件 if (params.getBrand() != null &amp;&amp; !params.getBrand().equals(\"\")) { boolQuery.filter(QueryBuilders.termQuery(\"brand\", params.getBrand())); } // 星级条件 if (params.getStarName() != null &amp;&amp; !params.getStarName().equals(\"\")) { boolQuery.filter(QueryBuilders.termQuery(\"starName\", params.getStarName())); } // 价格 if (params.getMinPrice() != null &amp;&amp; params.getMaxPrice() != null) { boolQuery.filter(QueryBuilders .rangeQuery(\"price\") .gte(params.getMinPrice()) .lte(params.getMaxPrice()) ); } // 2.算分控制 FunctionScoreQueryBuilder functionScoreQuery = QueryBuilders.functionScoreQuery( // 原始查询，相关性算分的查询 boolQuery, // function score的数组 new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{ // 其中的一个function score 元素 new FunctionScoreQueryBuilder.FilterFunctionBuilder( // 过滤条件 QueryBuilders.termQuery(\"isAD\", true), // 算分函数 ScoreFunctionBuilders.weightFactorFunction(10) ) }); request.source().query(functionScoreQuery);}" }, { "title": "分布式搜索引擎01", "url": "/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E01/", "categories": "微服务", "tags": "学习", "date": "2024-03-11 02:34:00 +0000", "snippet": "1.什么是elastisearch2.elasticsearch发展3.elasticsearch和mysql的区别（倒排索引和正排索引）1.分词器4.安装MQ安装elasticsearch1.部署单点es1.1.创建网络因为我们还需要部署kibana容器，因此需要让es和kibana容器互联。这里先创建一个网络：docker network create es-net1.2.加载镜像这里我...", "content": "1.什么是elastisearch2.elasticsearch发展3.elasticsearch和mysql的区别（倒排索引和正排索引）1.分词器4.安装MQ安装elasticsearch1.部署单点es1.1.创建网络因为我们还需要部署kibana容器，因此需要让es和kibana容器互联。这里先创建一个网络：docker network create es-net1.2.加载镜像这里我们采用elasticsearch的7.12.1版本的镜像，这个镜像体积非常大，接近1G。不建议大家自己pull。课前资料提供了镜像的tar包：大家将其上传到虚拟机中，然后运行命令加载即可：# 导入数据docker load -i es.tar同理还有kibana的tar包也需要这样做。1.3.运行运行docker命令，部署单点es：docker run -d \\\t--name es \\ -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\ -e \"discovery.type=single-node\" \\ -v es-data:/usr/share/elasticsearch/data \\ -v es-plugins:/usr/share/elasticsearch/plugins \\ --privileged \\ --network es-net \\ -p 9200:9200 \\ -p 9300:9300 \\elasticsearch:7.12.1命令解释： -e \"cluster.name=es-docker-cluster\"：设置集群名称 -e \"http.host=0.0.0.0\"：监听的地址，可以外网访问 -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"：内存大小 -e \"discovery.type=single-node\"：非集群模式 -v es-data:/usr/share/elasticsearch/data：挂载逻辑卷，绑定es的数据目录 -v es-logs:/usr/share/elasticsearch/logs：挂载逻辑卷，绑定es的日志目录 -v es-plugins:/usr/share/elasticsearch/plugins：挂载逻辑卷，绑定es的插件目录 --privileged：授予逻辑卷访问权 --network es-net ：加入一个名为es-net的网络中 -p 9200:9200：端口映射配置在浏览器中输入：http://192.168.150.101:9200 即可看到elasticsearch的响应结果：2.部署kibanakibana可以给我们提供一个elasticsearch的可视化界面，便于我们学习。2.1.部署运行docker命令，部署kibanadocker run -d \\--name kibana \\-e ELASTICSEARCH_HOSTS=http://es:9200 \\--network=es-net \\-p 5601:5601 \\kibana:7.12.1 --network es-net ：加入一个名为es-net的网络中，与elasticsearch在同一个网络中 -e ELASTICSEARCH_HOSTS=http://es:9200\"：设置elasticsearch的地址，因为kibana已经与elasticsearch在一个网络，因此可以用容器名直接访问elasticsearch -p 5601:5601：端口映射配置kibana启动一般比较慢，需要多等待一会，可以通过命令：docker logs -f kibana查看运行日志，当查看到下面的日志，说明成功：此时，在浏览器输入地址访问：http://192.168.150.101:5601，即可看到结果2.2.DevToolskibana中提供了一个DevTools界面：这个界面中可以编写DSL来操作elasticsearch。并且对DSL语句有自动补全功能。3.安装IK分词器3.1.在线安装ik插件（较慢）# 进入容器内部docker exec -it elasticsearch /bin/bash# 在线下载并安装./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.12.1/elasticsearch-analysis-ik-7.12.1.zip#退出exit#重启容器docker restart elasticsearch3.2.离线安装ik插件（推荐）1）查看数据卷目录安装插件需要知道elasticsearch的plugins目录位置，而我们用了数据卷挂载，因此需要查看elasticsearch的数据卷目录，通过下面命令查看:docker volume inspect es-plugins显示结果：[ { \"CreatedAt\": \"2022-05-06T10:06:34+08:00\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/es-plugins/_data\", \"Name\": \"es-plugins\", \"Options\": null, \"Scope\": \"local\" }]说明plugins目录被挂载到了：/var/lib/docker/volumes/es-plugins/_data 这个目录中。2）解压缩分词器安装包下面我们需要把课前资料中的ik分词器解压缩，重命名为ik3）上传到es容器的插件数据卷中也就是/var/lib/docker/volumes/es-plugins/_data ：4）重启容器# 4、重启容器docker restart es# 查看es日志docker logs -f es5）测试：IK分词器包含两种模式： ik_smart：最少切分 ik_max_word：最细切分 GET /_analyze{ \"analyzer\": \"ik_max_word\", \"text\": \"黑马程序员学习java太棒了\"}结果：{ \"tokens\" : [ { \"token\" : \"黑马\", \"start_offset\" : 0, \"end_offset\" : 2, \"type\" : \"CN_WORD\", \"position\" : 0 }, { \"token\" : \"程序员\", \"start_offset\" : 2, \"end_offset\" : 5, \"type\" : \"CN_WORD\", \"position\" : 1 }, { \"token\" : \"程序\", \"start_offset\" : 2, \"end_offset\" : 4, \"type\" : \"CN_WORD\", \"position\" : 2 }, { \"token\" : \"员\", \"start_offset\" : 4, \"end_offset\" : 5, \"type\" : \"CN_CHAR\", \"position\" : 3 }, { \"token\" : \"学习\", \"start_offset\" : 5, \"end_offset\" : 7, \"type\" : \"CN_WORD\", \"position\" : 4 }, { \"token\" : \"java\", \"start_offset\" : 7, \"end_offset\" : 11, \"type\" : \"ENGLISH\", \"position\" : 5 }, { \"token\" : \"太棒了\", \"start_offset\" : 11, \"end_offset\" : 14, \"type\" : \"CN_WORD\", \"position\" : 6 }, { \"token\" : \"太棒\", \"start_offset\" : 11, \"end_offset\" : 13, \"type\" : \"CN_WORD\", \"position\" : 7 }, { \"token\" : \"了\", \"start_offset\" : 13, \"end_offset\" : 14, \"type\" : \"CN_CHAR\", \"position\" : 8 } ]}3.3 扩展词词典随着互联网的发展，“造词运动”也越发的频繁。出现了很多新的词语，在原有的词汇列表中并不存在。比如：“奥力给”，“传智播客” 等。所以我们的词汇也需要不断的更新，IK分词器提供了扩展词汇的功能。1）打开IK分词器config目录：2）在IKAnalyzer.cfg.xml配置文件内容添加：&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 *** 添加扩展词典--&gt; &lt;entry key=\"ext_dict\"&gt;ext.dic&lt;/entry&gt;&lt;/properties&gt;3）新建一个 ext.dic，可以参考config目录下复制一个配置文件进行修改传智播客奥力给4）重启elasticsearchdocker restart es# 查看 日志docker logs -f elasticsearch日志中已经成功加载ext.dic配置文件5）测试效果：GET /_analyze{ \"analyzer\": \"ik_max_word\", \"text\": \"传智播客Java就业超过90%,奥力给！\"} 注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑3.4 停用词词典在互联网项目中，在网络间传输的速度很快，所以很多语言是不允许在网络上传递的，如：关于宗教、政治等敏感词语，那么我们在搜索时也应该忽略当前词汇。IK分词器也提供了强大的停用词功能，让我们在索引时就直接忽略当前的停用词汇表中的内容。1）IKAnalyzer.cfg.xml配置文件内容添加：&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典--&gt; &lt;entry key=\"ext_dict\"&gt;ext.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典 *** 添加停用词词典--&gt; &lt;entry key=\"ext_stopwords\"&gt;stopword.dic&lt;/entry&gt;&lt;/properties&gt;3）在 stopword.dic 添加停用词习大大4）重启elasticsearch# 重启服务docker restart elasticsearchdocker restart kibana# 查看 日志docker logs -f elasticsearch日志中已经成功加载stopword.dic配置文件5）测试效果：GET /_analyze{ \"analyzer\": \"ik_max_word\", \"text\": \"传智播客Java就业率超过95%,习大大都点赞,奥力给！\"} 注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑4.部署es集群部署es集群可以直接使用docker-compose来完成，不过要求你的Linux虚拟机至少有4G的内存空间首先编写一个docker-compose文件，内容如下：version: '2.2'services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.12.1 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data01:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.12.1 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.12.1 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elasticvolumes: data01: driver: local data02: driver: local data03: driver: localnetworks: elastic: driver: bridgeRun docker-compose to bring up the cluster:docker-compose up5.索引库操作1.索引操作2.文档的DSL操作修改文档6.RestClient操作索引库案例：注意：在springboot环境下用elastic search，一定要强制修改成7.12.1版本7.RestClient操作文档1.private RestHighLevelClient client;@BeforeEachvoid setup(){ this.client = new RestHighLevelClient(RestClient.builder( HttpHost.create(\"http://124.221.200.9:9200\") ));}@AfterEachvoid tearDown() throws IOException { this.client.close();}2.@Testvoid testAddDocument() throws IOException { Hotel hotel = hotelService.getById(61083L); HotelDoc hotelDoc = new HotelDoc(hotel); IndexRequest request = new IndexRequest(\"hotel\").id(hotel.getId().toString()); request.source(JSON.toJSONString(hotelDoc), XContentType.JSON); client.index(request, RequestOptions.DEFAULT);}3.4.5.总结8.批量操作document" }, { "title": "Mysql", "url": "/posts/MySQL/", "categories": "随笔", "tags": "学习", "date": "2024-03-11 02:34:00 +0000", "snippet": "1.存储引擎–视频地址：https://www.bilibili.com/video/BV1Kr4y1i7ru/1.存储引擎就是存储数据、建立索引、更新/查询数据等技术的实现方式。存储引擎是基于表的，而不是基于库的，所以存储引擎也可被称为表类型。2.存储引擎的特点事务的四大特性acid 原子性 隔离性 一致性 持久性3.索引概述介绍索引(index)是帮助MySQL高效获取数据的数据结构（有...", "content": "1.存储引擎–视频地址：https://www.bilibili.com/video/BV1Kr4y1i7ru/1.存储引擎就是存储数据、建立索引、更新/查询数据等技术的实现方式。存储引擎是基于表的，而不是基于库的，所以存储引擎也可被称为表类型。2.存储引擎的特点事务的四大特性acid 原子性 隔离性 一致性 持久性3.索引概述介绍索引(index)是帮助MySQL高效获取数据的数据结构（有序）。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。上面是二叉排序树1.二叉树索引2.通过红黑树来解决树的平衡问题：3.索引分类" }, { "title": "RabbitMQ", "url": "/posts/RabbitMQ/", "categories": "微服务", "tags": "学习", "date": "2024-03-10 02:34:00 +0000", "snippet": "1.初识RabbitMq同步调用存在的问题异步调用方案优势：2.什么是MQ国内一般选用1、3、4一般公司选用RabbitMQ，性能够用，可用性高，消息可靠性高都是他的亮点大公司选用kafka，单机吞吐量高，3.RabbitMQ快速入门4.SpringAMQP1.简单队列案例：2.WorkQueue 工作队列3.发布订阅1.FanoutExchange2.DirectExchange案例：步骤...", "content": "1.初识RabbitMq同步调用存在的问题异步调用方案优势：2.什么是MQ国内一般选用1、3、4一般公司选用RabbitMQ，性能够用，可用性高，消息可靠性高都是他的亮点大公司选用kafka，单机吞吐量高，3.RabbitMQ快速入门4.SpringAMQP1.简单队列案例：2.WorkQueue 工作队列3.发布订阅1.FanoutExchange2.DirectExchange案例：步骤一：3.TopicExchange案例：5.RabbitMQ部署指南1.单机部署我们在Centos7虚拟机中使用Docker来安装。1.1.下载镜像方式一：在线拉取docker pull rabbitmq:3-management方式二：从本地加载在课前资料已经提供了镜像包：上传到虚拟机中后，使用命令加载镜像即可：docker load -i mq.tar1.2.安装MQ执行下面的命令来运行MQ容器：docker run \\ -e RABBITMQ_DEFAULT_USER=itcast \\ -e RABBITMQ_DEFAULT_PASS=123321 \\ --name mq \\ --hostname mq1 \\ -p 15672:15672 \\ -p 5672:5672 \\ -d \\ rabbitmq:3-management2.集群部署接下来，我们看看如何安装RabbitMQ的集群。2.1.集群分类在RabbitMQ的官方文档中，讲述了两种集群的配置方式： 普通模式：普通模式集群不进行数据同步，每个MQ都有自己的队列、数据信息（其它元数据信息如交换机等会同步）。例如我们有2个MQ：mq1，和mq2，如果你的消息在mq1，而你连接到了mq2，那么mq2会去mq1拉取消息，然后返回给你。如果mq1宕机，消息就会丢失。 镜像模式：与普通模式不同，队列会在各个mq的镜像节点之间同步，因此你连接到任何一个镜像节点，均可获取到消息。而且如果一个节点宕机，并不会导致数据丢失。不过，这种方式增加了数据同步的带宽消耗。我们先来看普通模式集群。2.2.设置网络首先，我们需要让3台MQ互相知道对方的存在。分别在3台机器中，设置 /etc/hosts文件，添加如下内容：192.168.150.101 mq1192.168.150.102 mq2192.168.150.103 mq3并在每台机器上测试，是否可以ping通对方：" }, { "title": "Docker", "url": "/posts/Docker/", "categories": "微服务", "tags": "学习", "date": "2024-03-09 02:34:00 +0000", "snippet": "1.初识Docker2.镜像与容器 Docker架构 3.安装DockerDocker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。Docker CE 分为 stable test 和 nightly 三个更新频道。官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE...", "content": "1.初识Docker2.镜像与容器 Docker架构 3.安装DockerDocker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。Docker CE 分为 stable test 和 nightly 三个更新频道。官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 CentOS上的安装。1.CentOS安装DockerDocker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。1.1.卸载（可选）如果之前安装过旧版本的Docker，可以使用下面命令卸载：yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine \\ docker-ce1.2.安装docker首先需要大家虚拟机联网，安装yum工具yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 --skip-broken然后更新本地镜像源：# 设置docker镜像源yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i 's/download.docker.com/mirrors.aliyun.com\\/docker-ce/g' /etc/yum.repos.d/docker-ce.repoyum makecache fast然后输入命令：yum install -y docker-cedocker-ce为社区免费版本。稍等片刻，docker即可安装成功。1.3.启动dockerDocker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！启动docker前，一定要关闭防火墙后！！启动docker前，一定要关闭防火墙后！！启动docker前，一定要关闭防火墙后！！# 关闭systemctl stop firewalld# 禁止开机启动防火墙systemctl disable firewalld通过命令启动docker：systemctl start docker # 启动docker服务systemctl stop docker # 停止docker服务systemctl restart docker # 重启docker服务然后输入命令，可以查看docker版本：docker -v如图：1.4.配置镜像加速docker官方镜像仓库网速较差，我们需要设置国内镜像服务：参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors2.CentOS7安装DockerCompose2.1.下载Linux下需要通过命令下载：# 安装curl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose如果下载速度较慢，或者下载失败，可以使用课前资料提供的docker-compose文件：上传到/usr/local/bin/目录也可以。2.2.修改文件权限修改文件权限：# 修改权限chmod +x /usr/local/bin/docker-compose2.3.Base自动补全命令：# 补全命令curl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose如果这里出现错误，需要修改自己的hosts文件：echo \"199.232.68.133 raw.githubusercontent.com\" &gt;&gt; /etc/hosts3.Docker镜像仓库搭建镜像仓库可以基于Docker官方提供的DockerRegistry来实现。官网地址：https://hub.docker.com/_/registry3.1.简化版镜像仓库Docker官方的Docker Registry是一个基础版本的Docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。搭建方式比较简单，命令如下：docker run -d \\ --restart=always \\ --name registry\t\\ -p 5000:5000 \\ -v registry-data:/var/lib/registry \\ registry命令中挂载了一个数据卷registry-data到容器内的/var/lib/registry 目录，这是私有镜像库存放数据的目录。访问http://YourIp:5000/v2/_catalog 可以查看当前私有镜像服务中包含的镜像3.2.带有图形化界面版本使用DockerCompose部署带有图象界面的DockerRegistry，命令如下：version: '3.0'services: registry: image: registry volumes: - ./registry-data:/var/lib/registry ui: image: joxit/docker-registry-ui:static ports: - 8080:80 environment: - REGISTRY_TITLE=传智教育私有仓库 - REGISTRY_URL=http://registry:5000 depends_on: - registry3.3.配置Docker信任地址我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置：# 打开要修改的文件vi /etc/docker/daemon.json# 添加内容：\"insecure-registries\":[\"http://192.168.150.101:8080\"]# 重加载systemctl daemon-reload# 重启dockersystemctl restart docker4.镜像操作命令案例：从DockerHub中拉去一个nginx镜像并查看案例二：5.容器相关的常见命令让宿主机的80端口 和 容器的80端口 产生一个关联的映射（这样一来，任何进入宿主机80端口的请求，都会被转发到 容器的80端口 去执行请求）docker run --name mn -p 80:80 -d nginxdocker ps进入容器内部要想修改html页面中的文件，需要去docker官网去查看sed -i ‘s#&lt;head&gt;#&lt;head&gt;#g’ index.htmlsed -i ‘s#Welcome to nginx#superWang欢迎您#g’ index.html、注意：stop一个容器之后，再次启动的时候会报错，这时候重启容器systemctl restart docker再start就行了6.数据卷介绍挂载数据卷docker run –name mn -p 80:80 -v html:/usr/share/nginx/html -d nginx案列：docker run \\–name mysql -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -v /tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf -v /tmp/mysql/data:/var/lib/mysql -d mysql:5.7.25 优点：创建目录全权交给docker去处理，不用操心 缺点：不知道目录的具体位置，目录结构比较深，去找文件比较麻烦 7.自定义镜像docker build -t javaweb:1.0 . 最后是空格+. 相当于在当前目录进行操作如果是以java8为基础的话，可以指定基础镜像FROM java:8-alpineCOPY ./docker-demo.jar /tmp/app.jar暴露端口EXPOSE 8090入口，java项目的启动命令ENTRYPOINT java -jar /tmp/app.jar8.DockerCompose什么是DockerCompose它可以帮助我们快速部署分布式应用，无需一个个微服务去构建镜像和部署上传，给执行权限案例：9.创建私有镜像服务器" }, { "title": "微服务二-Feign", "url": "/posts/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BA%8C/", "categories": "微服务", "tags": "学习", "date": "2024-03-08 03:34:00 +0000", "snippet": "1.为什么要用Feign2.自定义Feign的配置第一种方式，基于配置文件第二种方式，基于java代码方式：总结3.Feign的性能优化具体实现：4.Feign的最佳实践缺点：不推荐服务端和客户端共享接口，因为他会造成紧耦合，而且这种继承方案对MVC不起作用，其中实现逻辑和athVariable不能省还得自己写一遍缺点：比如说order-service只需要两个方法，但是他把所有的方法都引过...", "content": "1.为什么要用Feign2.自定义Feign的配置第一种方式，基于配置文件第二种方式，基于java代码方式：总结3.Feign的性能优化具体实现：4.Feign的最佳实践缺点：不推荐服务端和客户端共享接口，因为他会造成紧耦合，而且这种继承方案对MVC不起作用，其中实现逻辑和athVariable不能省还得自己写一遍缺点：比如说order-service只需要两个方法，但是他把所有的方法都引过来了总结：使用第二种方法实践具体抽取步骤 先创建一个包，这里是Feign-api 在order-service中引入这个包 把order-service中的类转移到Feign-api中，然后删除order-service中的包， 此时的order-service的包会报错，我们要重新导如Feign-api中的包 Client中的包扫描不到，有两种解决办法 指定FeignClient所在包 指定FeignClient字节码 " }, { "title": "微服务三-网关", "url": "/posts/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%89/", "categories": "微服务", "tags": "学习", "date": "2024-03-08 02:34:00 +0000", "snippet": "1.网关功能springCloud中网管有两种 gateway zuul2.编写路由配置及nacos地址路由断言工厂过滤器工厂全局过滤器//@Order()@Componentpublic class AuthorizeFilter implements GlobalFilter, Ordered { @Override public Mono&lt;Void&gt; fil...", "content": "1.网关功能springCloud中网管有两种 gateway zuul2.编写路由配置及nacos地址路由断言工厂过滤器工厂全局过滤器//@Order()@Componentpublic class AuthorizeFilter implements GlobalFilter, Ordered { @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { //1.获取请求参数 ServerHttpRequest request = exchange.getRequest(); //2.获取参数中的 authorization参数 MultiValueMap&lt;String, String&gt; params = request.getQueryParams(); //3.判断参数值是否等于admin if (\"admin\".equals(params.getFirst(\"authorization\"))){ //4.是，放行 return chain.filter(exchange); } //5.否，拦截 //5.1设置状态码 exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); //5.2拦截请求 return exchange.getResponse().setComplete(); } @Override public int getOrder() { return -1; }}过滤器执行顺序统一网关跨域问题CORS：浏览器询问服务器，是否允许跨域，服务器会指定一些列配置，默认浏览器询问服务器发起的是option请求，但是gateway网关模式是禁止option访问的，所以要配置允许option" }, { "title": "微服务一Nacos", "url": "/posts/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%80/", "categories": "微服务", "tags": "学习", "date": "2024-03-07 02:34:00 +0000", "snippet": "1.微服务技术栈持续集成2.认识微服务单体架构分布式架构认识微服务springCloud与SpringBoot的版本兼容关系微服务拆分注意事项3.服务远程调用步骤一：步骤二：消费者与提供者4.Eureka注册中心服务调用会出现问题比如：这个时候需要Eureka可以解决这些问题Eureka的作用总结：如何搭建Eurekaeurka注册server: port: 10086 # 服务端口spr...", "content": "1.微服务技术栈持续集成2.认识微服务单体架构分布式架构认识微服务springCloud与SpringBoot的版本兼容关系微服务拆分注意事项3.服务远程调用步骤一：步骤二：消费者与提供者4.Eureka注册中心服务调用会出现问题比如：这个时候需要Eureka可以解决这些问题Eureka的作用总结：如何搭建Eurekaeurka注册server: port: 10086 # 服务端口spring: application: name: eurekaserver # eurka服务名称eureka: client: service-url: defaultZone: http://localhost:10086/eureka找到eureka的main函数，启动eureka服务的实例列表服务注册模拟多实例注入服务拉取Ribbon负载均衡5.Ribbon负载均衡负载均衡策略实现有两种方案6.Nacos启动Nacos报错-&gt;用这个命令解决.\\startup.cmd -m standalone访问此网站： http://192.168.150.1:8848/nacos/index.html网站默认账号和密码都是 nacos&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt;集群的概念：nacos引用此概念，就是为了防止服务跨集群调用\t如何配置一个实例的集群根据权重配置负载均衡7.环境隔离–namespacenacos注册中心细节分析临时实例和非临时实例nacos和eureka区别8.Nacos配置管理注：热更新需求，需要修改配置内容中的数据，切记不是把userservice中一切配置都拿过来，只需要把需要用到热更新的配置拿过来第一步：第二部：同意配置管理bootstrap.yml的优先级比nacos配置文件中的优先级更高，所以会优先访问bootstrap.yml文件name+active+file-extension = Date Id（nacos配置文件中）怎么证明读取到了nacos的配置？向配置中读取自定义的配置，如果读到了就等于配置成功了配置cocas注意：自动代码会出现discovery： 删除它就行了配置自动刷新方式一：方式二：多服务共享配置：9.Nacos集群搭建1.集群结构图官方给出的Nacos集群图：其中包含3个nacos节点，然后一个负载均衡器代理3个Nacos。这里负载均衡器可以使用nginx。我们计划的集群结构：三个nacos节点的地址： 节点 ip port nacos1 192.168.150.1 8845 nacos2 192.168.150.1 8846 nacos3 192.168.150.1 8847 2.搭建集群搭建集群的基本步骤： 搭建数据库，初始化数据库表结构 下载nacos安装包 配置nacos 启动nacos集群 nginx反向代理2.1.初始化数据库Nacos默认数据存储在内嵌数据库Derby中，不属于生产可用的数据库。官方推荐的最佳实践是使用带有主从的高可用数据库集群，主从模式的高可用数据库可以参考传智教育的后续高手课程。这里我们以单点的数据库为例来讲解。首先新建一个数据库，命名为nacos，而后导入下面的SQL：CREATE TABLE `config_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(255) DEFAULT NULL, `content` longtext NOT NULL COMMENT 'content', `md5` varchar(32) DEFAULT NULL COMMENT 'md5', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `src_user` text COMMENT 'source user', `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip', `app_name` varchar(128) DEFAULT NULL, `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', `c_desc` varchar(256) DEFAULT NULL, `c_use` varchar(64) DEFAULT NULL, `effect` varchar(64) DEFAULT NULL, `type` varchar(64) DEFAULT NULL, `c_schema` text, PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfo_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = config_info_aggr *//******************************************/CREATE TABLE `config_info_aggr` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(255) NOT NULL COMMENT 'group_id', `datum_id` varchar(255) NOT NULL COMMENT 'datum_id', `content` longtext NOT NULL COMMENT '内容', `gmt_modified` datetime NOT NULL COMMENT '修改时间', `app_name` varchar(128) DEFAULT NULL, `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfoaggr_datagrouptenantdatum` (`data_id`,`group_id`,`tenant_id`,`datum_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='增加租户字段';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = config_info_beta *//******************************************/CREATE TABLE `config_info_beta` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(128) NOT NULL COMMENT 'group_id', `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name', `content` longtext NOT NULL COMMENT 'content', `beta_ips` varchar(1024) DEFAULT NULL COMMENT 'betaIps', `md5` varchar(32) DEFAULT NULL COMMENT 'md5', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `src_user` text COMMENT 'source user', `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip', `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfobeta_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info_beta';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = config_info_tag *//******************************************/CREATE TABLE `config_info_tag` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(128) NOT NULL COMMENT 'group_id', `tenant_id` varchar(128) DEFAULT '' COMMENT 'tenant_id', `tag_id` varchar(128) NOT NULL COMMENT 'tag_id', `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name', `content` longtext NOT NULL COMMENT 'content', `md5` varchar(32) DEFAULT NULL COMMENT 'md5', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `src_user` text COMMENT 'source user', `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip', PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfotag_datagrouptenanttag` (`data_id`,`group_id`,`tenant_id`,`tag_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info_tag';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = config_tags_relation *//******************************************/CREATE TABLE `config_tags_relation` ( `id` bigint(20) NOT NULL COMMENT 'id', `tag_name` varchar(128) NOT NULL COMMENT 'tag_name', `tag_type` varchar(64) DEFAULT NULL COMMENT 'tag_type', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(128) NOT NULL COMMENT 'group_id', `tenant_id` varchar(128) DEFAULT '' COMMENT 'tenant_id', `nid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`nid`), UNIQUE KEY `uk_configtagrelation_configidtag` (`id`,`tag_name`,`tag_type`), KEY `idx_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_tag_relation';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = group_capacity *//******************************************/CREATE TABLE `group_capacity` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `group_id` varchar(128) NOT NULL DEFAULT '' COMMENT 'Group ID，空字符表示整个集群', `quota` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '配额，0表示使用默认值', `usage` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '使用量', `max_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个配置大小上限，单位为字节，0表示使用默认值', `max_aggr_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '聚合子配置最大个数，，0表示使用默认值', `max_aggr_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值', `max_history_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '最大变更历史数量', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_group_id` (`group_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='集群、各Group容量信息表';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = his_config_info *//******************************************/CREATE TABLE `his_config_info` ( `id` bigint(64) unsigned NOT NULL, `nid` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `data_id` varchar(255) NOT NULL, `group_id` varchar(128) NOT NULL, `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name', `content` longtext NOT NULL, `md5` varchar(32) DEFAULT NULL, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `src_user` text, `src_ip` varchar(50) DEFAULT NULL, `op_type` char(10) DEFAULT NULL, `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', PRIMARY KEY (`nid`), KEY `idx_gmt_create` (`gmt_create`), KEY `idx_gmt_modified` (`gmt_modified`), KEY `idx_did` (`data_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='多租户改造';/******************************************//* 数据库全名 = nacos_config *//* 表名称 = tenant_capacity *//******************************************/CREATE TABLE `tenant_capacity` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `tenant_id` varchar(128) NOT NULL DEFAULT '' COMMENT 'Tenant ID', `quota` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '配额，0表示使用默认值', `usage` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '使用量', `max_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个配置大小上限，单位为字节，0表示使用默认值', `max_aggr_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '聚合子配置最大个数', `max_aggr_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值', `max_history_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '最大变更历史数量', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='租户容量信息表';CREATE TABLE `tenant_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `kp` varchar(128) NOT NULL COMMENT 'kp', `tenant_id` varchar(128) default '' COMMENT 'tenant_id', `tenant_name` varchar(128) default '' COMMENT 'tenant_name', `tenant_desc` varchar(256) DEFAULT NULL COMMENT 'tenant_desc', `create_source` varchar(32) DEFAULT NULL COMMENT 'create_source', `gmt_create` bigint(20) NOT NULL COMMENT '创建时间', `gmt_modified` bigint(20) NOT NULL COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_tenant_info_kptenantid` (`kp`,`tenant_id`), KEY `idx_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='tenant_info';CREATE TABLE `users` (\t`username` varchar(50) NOT NULL PRIMARY KEY,\t`password` varchar(500) NOT NULL,\t`enabled` boolean NOT NULL);CREATE TABLE `roles` (\t`username` varchar(50) NOT NULL,\t`role` varchar(50) NOT NULL,\tUNIQUE INDEX `idx_user_role` (`username` ASC, `role` ASC) USING BTREE);CREATE TABLE `permissions` ( `role` varchar(50) NOT NULL, `resource` varchar(255) NOT NULL, `action` varchar(8) NOT NULL, UNIQUE INDEX `uk_role_permission` (`role`,`resource`,`action`) USING BTREE);INSERT INTO users (username, password, enabled) VALUES ('nacos', '$2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu', TRUE);INSERT INTO roles (username, role) VALUES ('nacos', 'ROLE_ADMIN');2.2.下载nacosnacos在GitHub上有下载地址：https://github.com/alibaba/nacos/tags，可以选择任意版本下载。本例中才用1.4.1版本：2.3.配置Nacos将这个包解压到任意非中文目录下，如图：目录说明： bin：启动脚本 conf：配置文件进入nacos的conf目录，修改配置文件cluster.conf.example，重命名为cluster.conf：然后添加内容：127.0.0.1:8845127.0.0.1.8846127.0.0.1.8847然后修改application.properties文件，添加数据库配置spring.datasource.platform=mysqldb.num=1db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user.0=rootdb.password.0=1232.4.启动将nacos文件夹复制三份，分别命名为：nacos1、nacos2、nacos3然后分别修改三个文件夹中的application.properties，nacos1:server.port=8845nacos2:server.port=8846nacos3:server.port=8847然后分别启动三个nacos节点：startup.cmd2.5.nginx反向代理找到课前资料提供的nginx安装包：解压到任意非中文目录下：修改conf/nginx.conf文件，配置如下：upstream nacos-cluster { server 127.0.0.1:8845;\tserver 127.0.0.1:8846;\tserver 127.0.0.1:8847;}server { listen 80; server_name localhost; location /nacos { proxy_pass http://nacos-cluster; }}而后在浏览器访问：http://localhost/nacos即可。代码中application.yml文件配置如下：spring: cloud: nacos: server-addr: localhost:80 # Nacos地址2.6.优化 实际部署时，需要给做反向代理的nginx服务器设置一个域名，这样后续如果有服务器迁移nacos的客户端也无需更改配置. Nacos的各个节点应该部署到多个不同服务器，做好容灾和隔离 " }, { "title": "MyBatisPlus", "url": "/posts/MybatisPlus/", "categories": "MyBatisPlus", "tags": "学习", "date": "2024-03-06 02:34:00 +0000", "snippet": "1.MyBatis基础1.常见注解2.mp约定3.自定义配置注意中有几种策略@TableField常见场景4.常见配置5.使用流程2.使用mp构造复杂的where条件自定义SQL3.Service接口Controller接口Service接口Iservice批量新增第三种是mysql驱动去做，是mysql的jar包去做，并不是mybatis去做，让mysql重新对数据进行处理使用方法：4.m...", "content": "1.MyBatis基础1.常见注解2.mp约定3.自定义配置注意中有几种策略@TableField常见场景4.常见配置5.使用流程2.使用mp构造复杂的where条件自定义SQL3.Service接口Controller接口Service接口Iservice批量新增第三种是mysql驱动去做，是mysql的jar包去做，并不是mybatis去做，让mysql重新对数据进行处理使用方法：4.mp代码生成工具 mybatisplus5.静态工具6.逻辑删除缺点枚举处理器1.加注解@EnunValue，告诉mp告诉哪个成员变量和数据库中的字段相对应2.让处理器生效利用@JsonValue确定返回值是什么类型，加在value上就是返回的数字，加在desc返回的就是正常/冻结，不加的话返回NORMAL/FROZEN7.Json处理器第一步给字段上定义一个处理器，第二步开启自动的ResultMap映射8.插件功能第一步：首先要在配置类中注册MyBatisPlus的核心插件，同时添加分页功能第二部：就可以使用分页的API了例子：" }, { "title": "WebSpcket", "url": "/posts/WebSocket/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-02-20 02:34:00 +0000", "snippet": "1.WebSocket", "content": "1.WebSocket" }, { "title": "Spring-Cache", "url": "/posts/Spring-Cache/", "categories": "随笔", "tags": "学习", "date": "2024-02-17 02:34:00 +0000", "snippet": "1.Spring Cache介绍2.常用注解静态设置key@CachePut(cacheNames = \"userCache\" key = \"abc\")//如果使用Sspring Cache缓存数据，key的生成: userCache : : abc动态设置keyUserController中插入写法一：CachePut( cacheMames = \"userCache\" key = “#u...", "content": "1.Spring Cache介绍2.常用注解静态设置key@CachePut(cacheNames = \"userCache\" key = \"abc\")//如果使用Sspring Cache缓存数据，key的生成: userCache : : abc动态设置keyUserController中插入写法一：CachePut( cacheMames = \"userCache\" key = “#user.id\")//如果使用Spring Cache缓存数据，key的生成:userCache..abcpublic User save(@RequestBody User user){写法二：/@CachePut(cacheNames = \"usercache\" ,key = \" #result.id\")//对象导航写法三：` @CachePut( cacheNames = “userCache” , key = “#po.id”)`写法四：(@CachePut( cacheNames = \"userCache\" , key = \"#a0.id\")写法五：@CachePut(cacheNames = \"userCache\" ,key = \" #root.args[0].id\")Mapper中代码插入@Insert(\"insert into user(name,age) values ( #{name}, {age})\")@options(useGeneratedKeys = true,keyProperty = \"id\")void insert(User user);查找@GetMapping v@Cacheable(cacheNames = \"userCache\" , key = \"#id\") / / key的生成: userCache ::10public User getById(Long id){User user = userMapper.getById(id);\"return user};删除@DeleteMappingv@CacheEvict(cacheNames = \"userCache\" ,key = \"#id\") // key的生成:userCache::1d]public void deleteById(Long id){userMapper.deleteById( id);}删除全部@DeleteMapping(\"/ delAll\")@CacheEvict(cacheNames = \"userCache\",allEntries = true)public void deleteAll(){userMapper.deleteAl1();}" }, { "title": "微信小程序", "url": "/posts/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-02-06 02:34:00 +0000", "snippet": "1.配置application.yml和application-dev.ymlwechat: appid: ${sky.wechat.appid} secret: ${sky.wechat.secret}wechat: appid: xxx secret: xxx2.登陆Controller代码@RestController@RequestMapping(\"/user/user\")@...", "content": "1.配置application.yml和application-dev.ymlwechat: appid: ${sky.wechat.appid} secret: ${sky.wechat.secret}wechat: appid: xxx secret: xxx2.登陆Controller代码@RestController@RequestMapping(\"/user/user\")@Slf4j@Api(tags = \"C端用户相关接口\")public class UserController { @Autowired private UserService userService; @Autowired private JwtProperties jwtProperties; /** * 登录 * @param userLoginDTO * @return */ @ApiOperation(value = \"登录\") @PostMapping(\"/login\") public Result&lt;UserLoginVO&gt; login(@RequestBody UserLoginDTO userLoginDTO){ log.info(\"微信用户登陆：{}\",userLoginDTO); //微信登陆 User user = userService.weLogin(userLoginDTO); //为微信用户生成jwt令牌 HashMap&lt;String, Object&gt; claims = new HashMap&lt;&gt;(); claims.put(JwtClaimsConstant.USER_ID,user.getId()); String token = JwtUtil.createJWT(jwtProperties.getUserSecretKey(), jwtProperties.getUserTtl(), claims); UserLoginVO userLoginVO = UserLoginVO.builder() .id(user.getId()) .openid((user.getOpenid())) .token(token) .build(); return Result.success(userLoginVO); }}" }, { "title": "新增菜品代码开发", "url": "/posts/%E6%96%B0%E5%A2%9E%E8%8F%9C%E5%93%81%E4%BB%A3%E7%A0%81%E5%BC%80%E5%8F%91/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-02-04 02:34:00 +0000", "snippet": "产品原型需求分析", "content": "产品原型需求分析" }, { "title": "公共字段自动填充", "url": "/posts/%E5%85%AC%E5%85%B1%E5%AD%97%E6%AE%B5%E8%87%AA%E5%8A%A8%E5%A1%AB%E5%85%85/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-02-04 02:34:00 +0000", "snippet": "公共字段自动填充自定义注解package com.sky.annotation;import com.sky.enumeration.OperationType;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionP...", "content": "公共字段自动填充自定义注解package com.sky.annotation;import com.sky.enumeration.OperationType;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface AutoFill { //数据库操作类型:UPDATE INSERT OperationType value();}自定义切面，实现公共字段自动填充package com.sky.aspect;import com.sky.annotation.AutoFill;import com.sky.constant.AutoFillConstant;import com.sky.context.BaseContext;import com.sky.enumeration.OperationType;import lombok.extern.slf4j.Slf4j;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.Signature;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.springframework.stereotype.Component;import java.lang.reflect.Method;import java.time.LocalDateTime;/** * 自定义切面，实现公共字段自动填充处理逻辑 */@Aspect@Component //表示将此类标记为Spring容器中的一个Bean@Slf4jpublic class AutoFillAspect { /** * 切入点 */ //mapper下面所有的类和所有的方法 @Pointcut(\"execution(* com.sky.mapper.*.*(..)) &amp;&amp; @annotation(com.sky.annotation.AutoFill)\") public void autoFillPointCut(){} /** * 前置通知,在通知中进行公共字段的赋值 */ @Before(\"autoFillPointCut()\") public void antoFill(JoinPoint joinPoint){ log.info(\"开始进行公共字段的自动填充》》》\"); //获取当前被拦截方法上的数据库操作类型 MethodSignature signature = (MethodSignature)joinPoint.getSignature(); AutoFill autoFill = signature.getMethod().getAnnotation(AutoFill.class);//获取方法上的注解对象 OperationType operationType = autoFill.value(); //获取到当前被拦截的方法的参数--实体对象 Object[] args = joinPoint.getArgs(); if(args[0] == null || args.length == 0){ return; } Object entity = args[0]; //准备赋值的数据 LocalDateTime localDateTime = LocalDateTime.now(); Long currentId = BaseContext.getCurrentId(); //根据当前不同的操作类型，为队形的属性通过反射来赋值 if(operationType == OperationType.INSERT){ //为4个公共字段赋值给 try { Method setCreateTime = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_CREATE_TIME, LocalDateTime.class); Method setCreateUser = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_CREATE_USER, Long.class); Method setUpdateTime = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_UPDATE_TIME, LocalDateTime.class); Method setUpdateUser = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_UPDATE_USER, Long.class); //通过反射为对象属性赋值 setCreateTime.invoke(entity,localDateTime); setCreateUser.invoke(entity,currentId); setUpdateTime.invoke(entity,localDateTime); setUpdateUser.invoke(entity,currentId); } catch (Exception e) { throw new RuntimeException(e); } //通过反射为对象属性赋值 } else if (operationType == OperationType.UPDATE) { try { //为2个公共字段赋值 Method setUpdateTime = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_UPDATE_TIME,LocalDateTime.class); Method setUpdateUser = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_UPDATE_USER, Long.class); //通过反射为对象属性赋值 setUpdateTime.invoke(entity,localDateTime); setUpdateUser.invoke(entity,currentId); } catch (Exception e) { throw new RuntimeException(e); } } }}在需要拦截的方法上添加注解" }, { "title": "JWT令牌技术", "url": "/posts/JWT%E4%BB%A4%E7%89%8C%E6%8A%80%E6%9C%AF/", "categories": "随笔", "tags": "学习", "date": "2024-01-21 02:34:00 +0000", "snippet": "1.会话跟踪方案对比2.JWT令牌介绍3.JWT令牌的使用 只要JWT报错了，这个密钥就不能使用了4.登陆成功案例登陆失败", "content": "1.会话跟踪方案对比2.JWT令牌介绍3.JWT令牌的使用 只要JWT报错了，这个密钥就不能使用了4.登陆成功案例登陆失败" }, { "title": "面试题总结", "url": "/posts/%E9%9D%A2%E8%AF%95/", "categories": "面试", "tags": "学习", "date": "2024-01-16 02:34:00 +0000", "snippet": "1.Sychronized的偏向锁、轻量级锁、重量级锁在jdk1.6版本之前，Sychronized只有重量级锁，而这种锁在使用时，会切换用户状态和操作系统的内核状态，大大提高了系统的性能，所以在后来加入了偏向锁、轻量级锁，这是为了更好的提升系统的性能2什么情况下Interger会自动拆箱在Java中，当Integer类型的对象需要转换为基本数据类型（如int、double等）时，会发生拆箱...", "content": "1.Sychronized的偏向锁、轻量级锁、重量级锁在jdk1.6版本之前，Sychronized只有重量级锁，而这种锁在使用时，会切换用户状态和操作系统的内核状态，大大提高了系统的性能，所以在后来加入了偏向锁、轻量级锁，这是为了更好的提升系统的性能2什么情况下Interger会自动拆箱在Java中，当Integer类型的对象需要转换为基本数据类型（如int、double等）时，会发生拆箱操作。以下是几种情况下Integer会进行拆箱： 调用基本数据类型的方法或操作符：例如，将Integer对象与其他基本数据类型相加、相减、相乘等操作时，Integer对象会自动拆箱成对应的基本数据类型。 赋值给基本数据类型变量：当将一个Integer对象赋值给int类型的变量时，会触发拆箱操作。 方法参数需要基本数据类型：如果一个方法的参数是基本数据类型，而你传递给它一个Integer对象作为实参，编译器会自动将Integer对象拆箱为对应的基本数据类型。 比较操作：当使用关系运算符（如==、!=、&lt;、&gt;等）对Integer对象进行比较时，会进行拆箱操作。需要注意的是，拆箱操作可能会引发NullPointerException异常，特别是当Integer对象为null时进行拆箱操作。因此，在拆箱之前，建议先进行null值的判断。3.String a = ”a“和String a= new String(“a”)创建字符串的区别 直接定义的String a = ”A”，是存储在字符串常量池中的，new String（“A”）是存储在堆内存中的 直接定义的String A = “A”，在编译阶段创建，newString（“A”）是在运行时才会创建的 直接定义的String A=”A”,在变量池中只有一个，但是newString（“A”）在堆内存中，只要时创建一个，就会生成一个对象4.jdk、jre、jvm的区别 jdk是程序员所使用的开发工具包，它包括了编译运行程序的各种工具和资源，包括Java编译器、Java运行时环境，以及常用的Java类库等 jre是java运行环境，用来运行java程序的字节码文件，jre中包括了jvm以及jvm所需要的各种工具和资源，与jdk不同的是，他是给普通用户使用的，普通用户只需要安装jre就可以运行java文件 jvm是java虚拟机，是jre的一部分，他是整个java实现跨平台的最核心的部分，负责运行字节码文件 我们写Java代码，⽤txt就可以写，但是写出来的Java代码，想要运⾏，需要先编译成字节码，那就需要编译器，⽽JDK中就包含了编译器javac，编译之后的字节码，想要运⾏，就需要⼀个可以执⾏字节码的程序，这个程序就是JVM（Java虚拟机），专⻔⽤来执⾏Java字节码的。 JVM在执⾏Java字节码时，需要把字节码解释为机器指令，⽽不同操作系统的机器指令是有可能不⼀样的，所以就导致不同操作系统上的JVM是不⼀样的，所以我们在安装JDK时需要选择操作系统。另外，JVM是⽤来执⾏Java字节码的，所以凡是某个代码编译之后是Java字节码，那就都能在JVM上运⾏5.面向对象的四大特性抽象：继承：封装：多态：6.接口和抽象类的区别7.HashMap的扩容机制1.负载因子从代码中我们可以看到，在向HashMap中添加元素过程中，如果 元素个数（size）超过临界值（threshold） 的时候，就会进行自动扩容（resize），并且，在扩容之后，还需要对HashMap中原有元素进行rehash，即将原来通中的元素重新分配到新的桶中。在HashMap中，临界值（threshold） = 负载因子（loadFactor） * 容量（capacity）。loadFactor是装载因子，表示HashMap满的程度，默认值为0.75f，也就是说默认情况下，当HashMap中元素个数达到了容量的3/4的时候就会进行自动扩容。（相见HashMap中傻傻分不清楚的那些概念）2.为什么要扩容呢扩容需要对其容量进行扩充，并且还要进行rehash，这个过程其实很耗时的，但是我们为什么还要扩容呢？这是因为根据hash函数计算出来的地址，有可能出现hash碰撞，HashMap是由数组+链表构成的，这个时候可以将数据放到链表上，我们可以把树一直挂到链表上，但是这时候有个问题，链表如果太长的话，那么数据的查询就像在链表上一样了，链表的查询速度非常低，所以为了保证HashMap的读取速度，我们要想办法尽量保证HashMap的冲突不要太高。3.如何保证HashMap的冲突不会太高呢？冲突太高无非两种情况： 数组长度太短 hash函数设计的不够合理，导致将数据分到同一个或者几个桶中，分配不均所以解决Hsah碰撞也是从这两方面入手。为了避免哈希碰撞，HashMap需要在合适的时候进行扩容，需要设置一个合适的负载因子4.负载因子设置多少合适呢？在JDK官方文档中有一段描述，一般来说，默认的负载因子（0.75）在时间和空间成本上提供了较好的权衡，所以最好的负载因子的值是0.758.Java锁机制 偏向锁：指的是当一个线程访问一个对象的时候，这个对象的对象头中mark word，有一个名为线程id的属性，这时线程id会记录下此线程的id，以后这个线程再进行访问的话，可以直接访问这个对象 轻量级锁：当有其他线程访问这个对象的时候，偏向锁会自动升级成轻量级锁，此时不会再用对象头中的线程id属性记录线程了，而是把mark word和lock record绑定起来，其他线程会进入自旋状态，这个自旋时间也不是固定时间，而是根据上一次在同一个锁上自选的时间和锁状态这两个条件决定的，这就是适应性自旋 重量级锁： 如果此时有其他的线程也想访问这个对象，那么轻量级锁会升级为重量级锁，这时候需要通过Monitor对线程进行控制直接使用monitor不行吗？可以的，但如果有多个线程使用同一个资源，但是他们没有竞争，线程1,1点使用，线程2,2点使用，没有竞争。如果还用monitor是不是有点杀鸡用牛刀了。为了提升性能，提出了轻量级锁。不再用对象关联monitor了，而是对象的mark word字段和线程栈的 lock record进行交换作为锁。如果有锁重入，就会创建多个lock record放入线程栈，那么问题有来了，如果重入多了，多次创建lock record不是也消耗性能吗？所以提出了偏向锁。对线头设置线程id,解决重入问题。在理清下这些锁在什么场景下使用，1，重量级锁：多个线程有竞争；2.轻量级锁：多个线程但是没有竞争，这点尤为重要，轻量级锁是解决不了锁竞争的。轻量级锁解决的问题是在多线程没有竞争下，仍旧关联monitor的问题。3. 偏向锁：只有一个线程，没有其他线程。共享锁和排他锁 共享锁（S锁）: 也称为读锁。 如果事务T对数据对象A加上S锁，则可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁。这保证了其他事务可以读取A，但在事务T释放对象A上的S锁之前不能对A做任何修改。 排他锁（X锁）： 也成为写锁。 事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁，这保证了其他事务在T释放A上的锁之前不能在读取和修改A。 二、队列的分类Java 中的这些队列可以从不同的维度进行分类，例如可以从阻塞和非阻塞进行分类，也可以从有界和无界进行分类，而本文将从队列的功能上进行分类，例如：优先队列、普通队列、双端队列、延迟队列等1、阻塞队列与非阻塞队列1.1 阻塞队列阻塞队列（Blocking Queue）提供了可阻塞的 put 和 take 方法，它们与可定时的 offer 和 poll 是等价的。如果队列满了 put 方法会被阻塞等到有空间可用再将元素插入；如果队列是空的，那么 take 方法也会阻塞，直到有元素可用。当队列永远不会被充满时，put 方法和 take 方法就永远不会阻塞。在java包”java.util.concurrent”中，提供六个实现了”BlockingQueue”接口的阻塞队列： ArrayBlockingQueue 用数组实现的有界阻塞队列； LinkedBlockingQueue 基于链表实现的有界阻塞队列 PriorityBlockingQueue是一个带优先级的队列，基于堆数据结构的； DelayQueue是在PriorityQueue基础上实现的，底层也是数组构造方法，是一个存放Delayed 元素的无界阻塞队列； SynchronousQueue 一个没有容量的队列 ，不会存储数据； LinkedBlockingDeque 是双向链表实现的双向并发阻塞队列；1.2非阻塞队列所有无Blocking Queue的都是非阻塞，并且它不会包含 put 和 take 方法。2、有界队列和无界队列2.1 有界队列是指有固定大小的队列，比如设定了固定大小的 ArrayBlockingQueue，又或者大小为 0 的 SynchronousQueue。2.2 无界队列指的是没有设置固定大小的队列，但其实如果没有设置固定大小也是有默认值的，只不过默认值是 Integer.MAX_VALUE。3、双端队列Deque是一个双端队列接口，继承自Queue接口，Deque的实现类是LinkedList、ArrayDeque、LinkedBlockingDeque，其中LinkedList是最常用的。4、优先队列优先队列（PriorityQueue）是一种特殊的队列，它并不是先进先出的，而是优先级高的元素先出队。 优先队列是根据二叉堆实现的。最大堆和最小堆。5、延迟队列延迟队列（DelayQueue）是基于优先队列 PriorityQueue 实现的，它可以看作是一种以时间为度量单位的优先的队列，当入队的元素到达指定的延迟时间之后方可出队。三、队列的使用场景最典型的就是线程池，不同的线程池都是基于不同的队列来实现多任务等待的。1.LinkedBlockingQueue使用场景：在项目的一些核心业务且生产和消费速度相似的场景中:订单完成的邮件/短信提醒。 订单系统中当用户下单成功后，将信息放入ArrayBlockingQueue中，由消息推送系统取出数据进行消息推送提示用户下单成功。如果订单的成交量非常大，那么使用ArrayBlockingQueue就会有一些问题，固定数组很容易被使用完，此时调用的线程会进入阻塞，那么可能无法及时将消息推送出去，所以使用LinkedBlockingQueue比较合适，但是要注意消费速度不能太低，不然很容易内存被使用完。2.PriorityBlockingQueue使用场景：在项目上存在优先级的业务：VIP排队购票 用户购票的时候，根据用户不同的等级，优先放到队伍的前面，当存在票源的时候，根据优先级分配3.DelayQueue使用场景 ：由于是基于优先级队列实现，但是它比较的是时间，我们可以根据需要去倒叙或者正序排列(一般都是倒叙，用于倒计时)。所以适用于：订单超时取消功能、网站刷题倒计时 用户下订单未支付时，超时则释放订单中的资源，如果取消或者完成支付，我们再将队列中的数据移除掉。4.SynchronousQueue使用场景：参考线程池newCachedThreadPool()。 如果我们不确定每一个来自生产者请求数量但是需要很快的处理掉，那么配合SynchronousQueue为每个生产者请求分配一个消费线程是最简洁的办法。cking Queue）提供了可阻塞的 put 和 take 方法，它们与可定时的 offer 和 poll 是等价的。如果队列满了 put 方法会被阻塞等到有空间可用再将元素插入；如果队列是空的，那么 take 方法也会阻塞，直到有元素可用。当队列永远不会被充满时，put 方法和 take 方法就永远不会阻塞。在java包”java.util.concurrent”中，提供六个实现了10.spring中依赖注入的方式有哪些三种依赖于注解的注入方法1.变量注入 优点：注入方式简单，非常简洁 缺点：注入的对象不能用final修饰，可能会导致循环依赖问题，并且启动的时候不会报错，只有在使用那个bean的时候才会报错2.构造器注入 优点：1.显示注入必须强制注入，通过强制指明依赖注入来保证这个类的运行，防止发生NullPointerException 2.注入的对象可以用final修饰 3.可以避免循环依赖问题，如果存在循环依赖的话，spring项目启动的时候就会报错 缺点：当有十几个甚至更多对象需要注入时，构造函数的代码臃肿，看起来不太舒服 3.setter方式注入 优点：1.依赖注入中使用的依赖对象是可选的，意思是注入的依赖对象是可以为NULL的2.允许在类构造器完成过后重新注入 缺点：注入的对象不能用fianl修饰11ConcurrentHashMapJDK1.7JDK1.8" }, { "title": "Redis", "url": "/posts/Redis/", "categories": "随笔", "tags": "学习", "date": "2024-01-16 02:34:00 +0000", "snippet": "1.简介三种形式使用Redis2.进阶命令D:\\software\\Redis&gt;redis-cli.exe127.0.0.1:6379&gt; exit远程连接D:\\software\\Redis&gt;redis-cli.exe -h localhost -p 6379localhost:6379&gt; keys *常用命令：服务端:1、前台启动　　./redis-server (ct...", "content": "1.简介三种形式使用Redis2.进阶命令D:\\software\\Redis&gt;redis-cli.exe127.0.0.1:6379&gt; exit远程连接D:\\software\\Redis&gt;redis-cli.exe -h localhost -p 6379localhost:6379&gt; keys *常用命令：服务端:1、前台启动　　./redis-server (ctrl + c 退出前台启动)2、后台启动　　./redis-server redis.conf 本机根据配置项启动文件redis-server D:\\software\\Redis\\redis.windows.conf3、查看redis后台是否正常启动 　　方式一:　　ps -ef grep redis 　　方式二:　　ps aux grep redis 　　方式三:　　客户端登录之后输入ping,如果能显示pong则代表后台启动成功.4关闭redis后台 　　方式一: 先使用ps -ef grep redis 或者是 ps aux grep redis来查看redis的端口,然后输入kill redis端口号来关闭redis后台. 　　方式二: ./redis-cli shutdown客户端:1、登录客户端　　./redis-cli -h 127.0.0.1 -p 6379　　注: 客户端登录之后输入ping,如果能显示pong则代表后台启动成功.2、退出客户端　　方式一: ctrl + c　　方式二: quit　　方式三: exit3.修改密码在配置文件里找到一下语句1234564.各种数据的特点字符串操作命令 SETNX key value1get nameGeekHoursetex code 30 1234OKget code1234get codenullsetnx key 1 itcastERR wrong number of arguments for ‘setnx’ commandsetnx key1 itcast1setnx key1 itheima0get key1itcast哈希操作命令hset 100 name xiaoming1hset 100 age 221hget 100 namexiaominghget 100 age22hdel 100 name1hset 100 name xiaoming1hkeys 100agenamehvals 10022xiaoming列表操作命令lpush mylist a,b,c1lpush mylist a b c4lrange mylist 0 -1cbaa,b,crpop mylista,b,crpop mylistallen mylist2集合操作命令有序集合操作命令5.缓存击穿、缓存穿透、缓存雪崩如何记忆？ 缓存击穿你也可以这样想。”击穿”中的”击”字与”寄”发音相同。在网络上，当我们说“我寄了”，它通常隐含着“我完了”或者“我死了”的意味。沿着这个思路，缓缓存击穿就好比是某个关键的key“寄”了，导致的大量并发请求打到数据库。所以，只要你能从“击”这个字联想到“寄寄”，便能够自然而然地想到缓存击穿这一概念的具体含义。 缓存穿透缓存穿透其实就是恶意攻击，就是有人想“穿”过你的防护网(缓存)来“偷”你的家(数据库)，这也就是穿透。或者你这样想，有人想恶意攻击你的数据库，这是不是不好的行为，是不是小偷的行为！这样透和偷，不就联系起来了。那么下次面试官问你缓存穿透是什么的时候。你只要想到透就是偷，就是不好的行为，也就能想到缓存穿透的意思就是别人恶意攻击，故意频繁访问一个不存在的key，来压垮你的数据库。 缓存雪崩其实，记住缓存雪崩相比另外两个是比较容易的。雪崩这个词本身就可以生动地描绘了缓存雪崩发生的情境：发挥你的想象，redis就是个雪山，而redis上面的key好比山上一个一个的雪花。突然间，一个自称“天下第一帅”的拙野挥洒着他的帅气，高声发出震天的呐喊：我是天下第一帅！。这一声犹如惊雷贯耳，瞬间引发了山上的积雪大面积崩塌(大量key失效)或者这声吼叫的力量如此巨大，竟像是触发了一场地震，使得整个雪山轰然倒塌（redis服务器宕机）。" }, { "title": "文件上传", "url": "/posts/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-01-14 02:34:00 +0000", "snippet": "1.文件上传，前端三要素分隔符文件上传这里必须选择公共读，否者后面无法通过默认路径访问到oos存储的文件，这个找了我好久，醉了不便维护第一种方法：对应的java代码第二种写法：application.ymlalioss: endpoint: oss-cn-hangzhou.aliyuncs.com access-key-id: LTAI5tNhb38yrRSKn1naqk16 ...", "content": "1.文件上传，前端三要素分隔符文件上传这里必须选择公共读，否者后面无法通过默认路径访问到oos存储的文件，这个找了我好久，醉了不便维护第一种方法：对应的java代码第二种写法：application.ymlalioss: endpoint: oss-cn-hangzhou.aliyuncs.com access-key-id: LTAI5tNhb38yrRSKn1naqk16 access-key-secret: 4cdRGvmeslvAYILS4DWPUI1BUPbrAB bucket-name: web-tias-sky-take-outAliOssProperties.javapackage com.sky.properties;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Component@ConfigurationProperties(prefix = \"sky.alioss\")@Datapublic class AliOssProperties { private String endpoint; private String accessKeyId; private String accessKeySecret; private String bucketName;}创建配置类，把对象创建出来，在项目启动时就能加载到package com.sky.config;import com.sky.properties.AliOssProperties;import com.sky.utils.AliOssUtil;import lombok.extern.slf4j.Slf4j;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * 配置类，用于创建AliOssUtil对象 */@Configuration@Slf4jpublic class OssConfiguration { @Bean @ConditionalOnMissingBean public AliOssUtil aliOssUtil(AliOssProperties aliOssProperties){ log.info(\"开始创建阿里云文件上传工具类对象：{}\",aliOssProperties); return new AliOssUtil(aliOssProperties.getEndpoint(), aliOssProperties.getAccessKeyId(), aliOssProperties.getAccessKeySecret(), aliOssProperties.getBucketName()); }}" }, { "title": "分页查询", "url": "/posts/%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/", "categories": "JavaWeb基础", "tags": "学习", "date": "2024-01-14 02:34:00 +0000", "snippet": "1.分页查询语法", "content": "1.分页查询语法" }, { "title": "MySQL数据库", "url": "/posts/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/", "categories": "随笔", "tags": "学习", "date": "2024-01-12 02:34:00 +0000", "snippet": "1.隐式内连接和显示内连接起别名2.事务3.四大特性4.索引create index 索引名 on 表名(字段名);优缺点吧结构", "content": "1.隐式内连接和显示内连接起别名2.事务3.四大特性4.索引create index 索引名 on 表名(字段名);优缺点吧结构" }, { "title": "MyBatis", "url": "/posts/MyBatis/", "categories": "JavaWeb基础", "tags": "学习", "date": "2024-01-12 02:34:00 +0000", "snippet": "1.什么是MyBatis官网：MyBatis中文网2.利用MyBatis操作数据库 在IDEA中配置数据库创建SpringBoot项目创建pojo.User实体类创建mapper.UserMapper接口测试先写接口，然后再写测试类配置SQL语句在IDEA中配置Mysql数据库JDBC程序MyBatis程序3.数据库连接池如何切换连接池 直接在依赖里面加入德鲁伊依赖成功转换成德鲁伊4.l...", "content": "1.什么是MyBatis官网：MyBatis中文网2.利用MyBatis操作数据库 在IDEA中配置数据库创建SpringBoot项目创建pojo.User实体类创建mapper.UserMapper接口测试先写接口，然后再写测试类配置SQL语句在IDEA中配置Mysql数据库JDBC程序MyBatis程序3.数据库连接池如何切换连接池 直接在依赖里面加入德鲁伊依赖成功转换成德鲁伊4.lombok问题分析具体使用springboot中已经集成了lombok的版本，并且进行了统一的管理，这里就不需要再说明版本&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt;5.MyBatis基础操作-环境准备注意：实体类和数据库中定义的名称有差别删除操作mybatis日志输出预编译SQLSQL注入以下列子就是预编译新增员工更新员工数据查询 方案一：给字段起别名，让别名与实体类一致 方案二：通过@Results，@Result注解手动映射封装 方案三：开启mybatis的驼峰命名的自动开关 —a Cloumn\t\t————-&gt;aColumn 模糊查询需要改进 //根据条件查询员工 @Select(\"select * from emp where name like concat('%',#{name},'%')and gender = #{gender} and \" + \"entrydate between #{begin} and #{end} order by id desc \") public List&lt;Emp&gt; list(String name, Short gender, LocalDate begin, LocalDate end);}6.XML映射文件用/斜线分隔namespace的值要和映射文件中的类名相同MyBatisX下载安装MyBatisX动态SQLif如果第一个参数为空，后面参数不为空，会出现以下错误 用where标签 where标签的两个优点 where元素只会在子元素有内容的情况下才插入where子句。而且会自动去除子句的开头的AND或OR。updateset标签的两个有点 动态的在行首插入SET关键字，并会删除额外的逗号。（用在update语句中）foreach&lt;!--批量删除员工（18，19，20 --&gt;&lt;!-- collection:遍历的集合 iten:遍历出来的元素 separator:分隔符 open:遍历开始前拼接的SQL片段 close:遍历结束后拼接的片段 --&gt;&lt;delete id=\"deleteByIds\"&gt; delete from emp where id in &lt;foreach collection=\"ids\" item=\"id\" separator=\",\" open=\"(\" close=\")\"&gt; #{id} &lt;/foreach&gt;&lt;/delete&gt;sql片段" }, { "title": "网络通信基础", "url": "/posts/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%9F%BA%E7%A1%80/", "categories": "随笔", "tags": "学习", "date": "2024-01-10 02:34:00 +0000", "snippet": "1.IP地址ip域名2.端口3.UDP和TCP协议TCP协议三次握手四次挥手4.UDP通信编程客户端代码服务端代码多开客户端5.TCP通信TCP客户端通讯TCP服务端程序开发案例一发一收客服端服务端以上程序一对一通信，只会接受一个客户端通信。因为连接客户端之后，程序会一直等待该客户端所传来的信息，并不会读取心得客户端。多客户端通信更新：端口转发，一对多BS架构如何优化默认执行插入是不会返回主键值的", "content": "1.IP地址ip域名2.端口3.UDP和TCP协议TCP协议三次握手四次挥手4.UDP通信编程客户端代码服务端代码多开客户端5.TCP通信TCP客户端通讯TCP服务端程序开发案例一发一收客服端服务端以上程序一对一通信，只会接受一个客户端通信。因为连接客户端之后，程序会一直等待该客户端所传来的信息，并不会读取心得客户端。多客户端通信更新：端口转发，一对多BS架构如何优化默认执行插入是不会返回主键值的" }, { "title": "Web入门", "url": "/posts/Web%E5%85%A5%E9%97%A8/", "categories": "JavaWeb基础", "tags": "学习", "date": "2024-01-09 02:34:00 +0000", "snippet": "1.SpringBootWeb快速入门注意：java版本与jdk版本不一致 创建springboot时遇到的问题，本人用的是1.8版本的jdk，但是java对应的版本只有17/21 解决办法：我们可以通过阿里云国服去间接创建Spring项目 参考链接：IDEA2023版本创建Sping项目只能勾选17和21，却无法使用Java8？（已解决）_spring initializ...", "content": "1.SpringBootWeb快速入门注意：java版本与jdk版本不一致 创建springboot时遇到的问题，本人用的是1.8版本的jdk，但是java对应的版本只有17/21 解决办法：我们可以通过阿里云国服去间接创建Spring项目 参考链接：IDEA2023版本创建Sping项目只能勾选17和21，却无法使用Java8？（已解决）_spring initializr不能选择java8-CSDN博客注意：创建不了java.class文件这里没有创建Java选项的可以在目录Java那里点击鼠标右键选择将目录标记为——»源代码根目录2.创建springboot新建一个controller.hello.class,并编写以下代码，在浏览器说如localhost:8080/hello就可以访问到3.HTTp协议状态嘛大全： [HTTP - 状态 Status - 开发者手册 - 腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/chapter/13553) 4.Web服务器如何将项目布置到tomcat中启动tomcat访问Tomcat介绍dispatcherServlet叫做核心控制器或者前端控制器请求响应原始方式SpringBoot方式总结简单实体对象复杂实体对象数组集合参数集合请求方式日期参数JSON参数利用RequestBody注解响应数据 不便管理 难以维护统一的响应结果请求-响应小demo 复用性差 难以维护分层解耦IOC&amp;DI入门 这个对象怎么交给容器进行管理–》控制反转 容器怎么为程序提供它所依赖的资源–》依赖注入 如果要切换实现类，只要将你想要切换的那个类加上@Component注解，将原来类的注解删掉就行IOC详解Bean组件扫描扫描不到包，有两个解决办法 在启动程序中，手动设置包扫描，手动设置之后，之前的包也需要手动设置一遍，比如：com.itheima 把所有包都放在启动类所在包的下面Di详解springboot中Autowired注解是什么作用？在Spring Boot中，@Autowired 是一个用于自动装配DI的注解。它用于告诉Spring容器在需要某个类型的Bean时，通过自动装配的方式来提供该类型的实例。动态sql写法select&lt;select id=\"page\" resultType=\"SetMeal\"&gt; select * from setmeal &lt;where&gt; &lt;if test=\"name != null\"&gt; and name like concat('%',#{name},'%') &lt;/if&gt; &lt;if test=\"categoryId != null\"&gt; and category_Id = #{categoryId}&lt;/if&gt; &lt;if test=\"status != null\"&gt; and status = #{status}&lt;/if&gt; &lt;/where&gt; order by create_time desc&lt;/select&gt;update&lt;update id=\"updateDish\" useGeneratedKeys=\"true\" keyProperty=\"id\" parameterType=\"Dish\"&gt; update dish &lt;set&gt; &lt;if test=\"name != null\"&gt;name = #{name},&lt;/if&gt; &lt;if test=\"categoryId != null\"&gt;category_Id = #{categoryId},&lt;/if&gt; &lt;if test=\"price != null\"&gt;price = #{price},&lt;/if&gt; &lt;if test=\"image != null\"&gt;image = #{image},&lt;/if&gt; &lt;if test=\"description != null\"&gt;description = #{description},&lt;/if&gt; &lt;if test=\"createTime != null\"&gt;create_Time = #{createTime},&lt;/if&gt; &lt;if test=\"updateTime != null\"&gt;update_Time = #{updateTime},&lt;/if&gt; &lt;if test=\"createUser != null\"&gt;create_User = #{createUser},&lt;/if&gt; &lt;if test=\"updateUser != null\"&gt;update_User = #{updateUser},&lt;/if&gt; &lt;if test=\"status != null\"&gt;status =#{status},&lt;/if&gt; &lt;/set&gt; where id = #{id}&lt;/update&gt;" }, { "title": "JavaWeb学习路线", "url": "/posts/JavaWeb%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/", "categories": "JavaWeb基础", "tags": "学习", "date": "2024-01-09 02:34:00 +0000", "snippet": "1.学习路线2.网站的开发模式3.初始Web程序", "content": "1.学习路线2.网站的开发模式3.初始Web程序" }, { "title": "JAVA异常积累", "url": "/posts/JAVA%E5%BC%82%E5%B8%B8%E7%A7%AF%E7%B4%AF/", "categories": "随笔", "tags": "学习", "date": "2024-01-09 02:34:00 +0000", "snippet": "1.org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.sky.mapper.EmployeeMapper.pageQuery]这个错误表明在处理请求时发生了异常，异常的根本原因是 MyBatis 的绑定异常（BindingException）。具体来说，错误信息显示了一个无...", "content": "1.org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.sky.mapper.EmployeeMapper.pageQuery]这个错误表明在处理请求时发生了异常，异常的根本原因是 MyBatis 的绑定异常（BindingException）。具体来说，错误信息显示了一个无效的绑定语句（Invalid bound statement），即在 MyBatis 中找不到指定的语句。2.“java: 警告: 源发行版 11 需要目标发行版 11”错误解决“java: 警告: 源发行版 11 需要目标发行版 11”错误解决-CSDN博客3.SpringBoot项目启动报 Disconnected from the target VM 错误SpringBoot项目启动报 Disconnected from the target VM 错误_disconnected from the target vm, address: ‘javadeb-CSDN博客4.IDEA| Spring Boot项目中@SpringBootTest测试的时候卡住，一直Resolving Juint-platform-engine/1.6.2…….. [IDEA Spring Boot项目中@SpringBootTest测试的时候卡住，一直Resolving Juint-platform-engine/1.6.2…….._idea的pom@test为什么一直加载-CSDN博客](https://blog.csdn.net/Cobbyer/article/details/106601619) 5.java: 程序包org.springframework.boot不存在 上网找了教程说在setting里面的runner勾上托管给maven 但是当我在mybatis中运行insert会报重复插入的错误，搜索的结果让我去掉Maven工程依赖 继续搜找到这篇文章报错 java: 程序包org.springframework.boot不存在 的一个解决办法-CSDN博客 解决方法是：清除IDEA缓存，然后重启IDEA6.新版IDEA无法创建二级包问题新版IDEA无法创建二级包问题（已解决）_idea中二级包为什么创建不了-CSDN博客7.解决办法：git错误 error: failed to push some refs to ‘https://github.com/…解决办法：git错误 error: failed to push some refs to ‘https://github.com/…-CSDN博客8.mybatis使用xmlspringboot整合mybatis使用xml实现sql语句的配置,在insert之后返回自增id_db-column-underline-CSDN博客9.SpringBoot Controller接收参数的几种常用方式SpringBoot Controller接收参数的几种常用方式_spring boot 接收参数-CSDN博客" }, { "title": "员工功能开发", "url": "/posts/%E5%91%98%E5%B7%A5%E5%8A%9F%E8%83%BD%E5%BC%80%E5%8F%91/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-01-07 02:34:00 +0000", "snippet": "实际开发过程代码开发注意：当前端提交的数据和实体类中对应的属性差异比较大时，建议使用DTO来进行封装数据idea连接数据库教程超详细的Idea与MySQL的连接（从入门到精通）_idea连接数据库-CSDN博客梳理过程1.2.3.功能测试但是此时有jwt校验,我们测试的jwt为空，需要加入全局jwt测试请求参数名称并没有写死，而是放在了这个文件夹下，请求参数要和令牌名称一致代码完善 ...", "content": "实际开发过程代码开发注意：当前端提交的数据和实体类中对应的属性差异比较大时，建议使用DTO来进行封装数据idea连接数据库教程超详细的Idea与MySQL的连接（从入门到精通）_idea连接数据库-CSDN博客梳理过程1.2.3.功能测试但是此时有jwt校验,我们测试的jwt为空，需要加入全局jwt测试请求参数名称并没有写死，而是放在了这个文件夹下，请求参数要和令牌名称一致代码完善 处理抛出异常 后端成功捕获错误2.如何获得当前登陆ID呢，这需要了解jwt令牌认证流程小技巧，选中需要计算的代码，然后运行运行之后，发现编号添加完成最后一步，提交推送需求分析和设计controller中代码service中代码Mapper以及映射文件代码代码完善数据格式不是我们想要的建议使用第二次方式，只需要加一次就行了对比图方式二原理是创建消息转换器对象，然后再把json序列化对象加入其中最后把消息转化器对象设为0的位置，也就是首位，确保系统会首先使用我们制定的消息转化器" }, { "title": "苍穹外卖-前后端环境搭建", "url": "/posts/%E8%8B%8D%E7%A9%B9%E5%A4%96%E5%8D%96-%E5%89%8D%E5%90%8E%E7%AB%AF%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/", "categories": "苍穹外卖项目实战", "tags": "学习", "date": "2024-01-06 02:34:00 +0000", "snippet": "1.后端搭建环境2.请求地址不一样，为什么能正常访问服务器3.实际上是利用Nginx转发给后端这样做的优点是：4.如何配置Nginx Nginx会匹配到api，然后把动态请求路径加在后面转发给tomcat服务器5.负载均衡配置 负载服务器可以分配客户端请求登陆功能完善1.完善登陆功能前后端分离开发注意接口文档很重要，优点1.有利于前后端并行开发 2.后端自测，缺点1.需要不断完善，耗时较...", "content": "1.后端搭建环境2.请求地址不一样，为什么能正常访问服务器3.实际上是利用Nginx转发给后端这样做的优点是：4.如何配置Nginx Nginx会匹配到api，然后把动态请求路径加在后面转发给tomcat服务器5.负载均衡配置 负载服务器可以分配客户端请求登陆功能完善1.完善登陆功能前后端分离开发注意接口文档很重要，优点1.有利于前后端并行开发 2.后端自测，缺点1.需要不断完善，耗时较长导入接口文档 登陆此网站 2.创建两个项目3.导入接口Swagger1.介绍2.使用方式1.2.3.3.重新启动会看到日志打印信息，说明程序没有问题4，访问已经生成的接口文档苍穹外卖项目接口文档5.所采的坑 注意：设置静态映射必须要有，如果没有的话，spring会认为这是一个动态请求，会在control类里面寻找接口， 扫描包的路径必须正确，否则扫面不出来东西6.Swagger常用注解使用注解之后会产生各个参数的解释更方便开发时进行阅读" }, { "title": "Linux命令", "url": "/posts/linux%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/", "categories": "随笔", "tags": "学习", "date": "2024-01-06 02:34:00 +0000", "snippet": "常见的Linux命令进阶版", "content": "常见的Linux命令进阶版" }, { "title": "Maven学习", "url": "/posts/Maven%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/", "categories": "JavaWeb基础", "tags": "学习", "date": "2024-01-06 02:34:00 +0000", "snippet": "1.Maven介绍通过提供各种各样的插件来完成项目的标准化构建 当钱Maven工程归属的组织 模块名称 版本私服的使用 优先使用私服的依赖 私服没有的依赖，会从中央仓库下载 只要公司中有一个人下载了依赖，其他人就不需要再下载了本地仓库地址，在maven的config\\setting.xml中修改整体安装配置Maven环境（全局）改成和jdk对应的版本2.创建一个Maven项目Ma...", "content": "1.Maven介绍通过提供各种各样的插件来完成项目的标准化构建 当钱Maven工程归属的组织 模块名称 版本私服的使用 优先使用私服的依赖 私服没有的依赖，会从中央仓库下载 只要公司中有一个人下载了依赖，其他人就不需要再下载了本地仓库地址，在maven的config\\setting.xml中修改整体安装配置Maven环境（全局）改成和jdk对应的版本2.创建一个Maven项目Maven坐标导入Maven工程的两种方式1.2.注意：Maven在编译时出现如下错误Failed to execute goal on project XXXX: Could not resolve dependencies for project XXXX: Failed to collect dependencies at XXXX出现这样的错误，是因为自己没有首先对父项目也就是sky-take-out项目进行clean和install ，执行父项目sky-take-out的clean和install，3.依赖管理添加依赖第一次使用依赖，怎么引用呢？ 打开此网站：Maven Repository: Search/Browse/Explore (mvnrepository.com) 搜索logback（所需依赖） 依赖传递依赖可视化查看排除依赖依赖范围生命周期" }, { "title": "git命令的使用", "url": "/posts/git%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8/", "categories": "随笔", "tags": "学习", "date": "2024-01-03 02:34:00 +0000", "snippet": "git几种状态git reset的三种模式 git reset – soft 工作区和暂存区回说某一个版本，不会删除工作区和暂存区的内容 git reset –hard 工作区和暂存区回溯上某个版本，会删除工作区和暂存区的内容 git reset – mixed 工作区和暂存区回溯上某个版本，不会删除工作区的内容，但会删除暂存区的内容git diff的使用git中有工作区、暂存区和版...", "content": "git几种状态git reset的三种模式 git reset – soft 工作区和暂存区回说某一个版本，不会删除工作区和暂存区的内容 git reset –hard 工作区和暂存区回溯上某个版本，会删除工作区和暂存区的内容 git reset – mixed 工作区和暂存区回溯上某个版本，不会删除工作区的内容，但会删除暂存区的内容git diff的使用git中有工作区、暂存区和版本库 git diff head(比较工作区和暂存区的差异)\thead表示当前分支的最新提交 git diff –cached(比较暂存区和版本库之间的差异) git diff其他用法 ~2表示head之前的两个版本git删除操作.gitignore文件的匹配规则列子ssh配置和克隆仓库git关于远程仓库的使用在vscode中使用gitgit中分支操作git合并冲突如何解决 出现如下错误 使用 git status查看其状态 3.使用 git diff查看需要修改的地方4.利用notepad main2.txt打开文件，去掉+&lt;&gt;=号后将两个句子合并到一起，最后再add和commit和git commit -m”merg”git中rebase的使用git回溯git定义别名保持良好的git命名习惯" }, { "title": "JAVA基础", "url": "/posts/JAVA%E5%9F%BA%E7%A1%80/", "categories": "随笔", "tags": "学习", "date": "2024-01-02 02:34:00 +0000", "snippet": "1.IDEA常用快捷键2.匿名内部类3.枚举4.泛型自定义泛型类实际实现泛型 接口5.接口接口的好处这里new的不是接口，而是A的实例，前面指的是定义了一个接口变量，后面指的是用A的实例（对象）去指向改接口变量。简而言之，接口不能实例化对象，但是可以创建一个变量，让他的实现类的实例指向该接口 jdk8时为了能够使用接口的升级，什么是接口升级呢，当项目中的一个接口，需要加新的抽象方法...", "content": "1.IDEA常用快捷键2.匿名内部类3.枚举4.泛型自定义泛型类实际实现泛型 接口5.接口接口的好处这里new的不是接口，而是A的实例，前面指的是定义了一个接口变量，后面指的是用A的实例（对象）去指向改接口变量。简而言之，接口不能实例化对象，但是可以创建一个变量，让他的实现类的实例指向该接口 jdk8时为了能够使用接口的升级，什么是接口升级呢，当项目中的一个接口，需要加新的抽象方法，那么原来实现了这个接口的鹅方法，也需要一起进行修改，这个时候我们就想到了定义一个默认方法，拥有这个方法的接口，在子类实现的时候可以选择性的进行实现，可以先实现abstrat修饰符的接口，回头再实现default修饰符的方法，但是有一点要注意，重新的时候要去掉default关键字 public可以省略，但是default不能省略 如果实现了多个接口，多个接口中存在相同的默认方法，子类就必须对该方法进行重写 二： static关键字 # jdk9新增的方法 使用private修饰的代码，是提供给接口中其他方法使用的方法，并不会被子类继承 public static的抽象方法只能调用private static方法6.权限修饰符 这里要注意的是，protected只能在子类中调用父类的方法 ，而无法在其它类中创造子类，然后在调用该方法7.子类访问成员的特点8.多态识别技巧： 对于方法，编译看左边，运行看右边 对于变量，编译看左边，运行看左边使用多态优点如何解决以上问题呢？ 可以把人的类型强制转化为学生的类型，就能调用学生类型独有的方法了 又存在一个问题，如果学生强转化为老师，带代码的时候不会报错，但是编译的时候会有错误。 总结：9.final、常量以下三点：常量： final 关键字可以用于成员变量、本地变量、方法以及类 final 成员变量必须在声明的时候初始化或者在构造器中初始化，否则就汇报编译错误 不能够对 final 变量再次赋值 本地变量必须在声明时赋值 在匿名类中所有变量都必须是 final 变量 final 方法不能被重写 final 类不能被继承 接口中声明的所有变量本身是 final 的 final 和 abstract 这两个关键字是反相关的，final 类就不可能是 abstract 的 没有在声明时初始化 final 变量的称为空白 final 变量(blank final variable)，它们必须在构造器中初始化，或者调用 this() 初始化，不这么做的话，编译器会报错final变量(变量名)需要进行初始化 按照 Java 代码惯例，final 变量就是常量，而且通常常量名要大写 对于集合对象声明为 final 指的是引用不能被更改10.抽象类11.Set集合的特点12.注解注解的原理元注解指的是：修饰注解的注解实现方式作用：声明注解的保留周期13.关键字static：" } ]
